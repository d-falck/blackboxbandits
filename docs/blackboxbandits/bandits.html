<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>blackboxbandits.bandits API documentation</title>
<meta name="description" content="Provides implementations of various bandit algorithms in a generic context." />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>blackboxbandits.bandits</code></h1>
</header>
<section id="section-intro">
<p>Provides implementations of various bandit algorithms in a generic context.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;Provides implementations of various bandit algorithms in a generic context.
&#34;&#34;&#34;

from abc import ABC, abstractmethod
from typing import List, Optional
import numpy as np


class AbstractMultiBandit(ABC):
    &#34;&#34;&#34;Abstract base class for multi-bandits.
    
    Any instance will repeatedly choose a fixed number of arms to pull
    and then observe the rewards for each of those arms, receiving the maximum
    of those rewards.
    
    Parameters
    ----------
    A : int
        Number of arms available to choose from. Arms are subsequently referred
        to by their zero-indexed number.
    T : int
        Arm budget at each round, i.e. how many arms the bandit is allowed to
        pull.
    n : int
        Number of rounds expected overall. Used by some children.

    Attributes
    ----------
    Same as parameters, plus:
    self.round : int
        The current round, between `0` and `n`.
    self.expecting_observation : bool
        Whether or not the bandit is currently waiting to receive the rewards
        for the arms it just chose.
    &#34;&#34;&#34;

    def __init__(self, A: int, T: int, n: int):
        self.A = A
        self.T = T
        self.n = n
        self.round = 0
        self.expecting_observation = False

    @abstractmethod
    def select_arms(self) -&gt; List[int]:
        &#34;&#34;&#34;Get arms selected by the bandit at the current round.

        Must only be called when the bandit isn&#39;t currently waiting for rewards.

        Returns
        -------
        List[int]
            A list of length `T` of the arms chosen by the bandit to pull at
            this round. Identified by their zero-indexed number.
        &#34;&#34;&#34;
        assert self.round &lt; self.n, \
            &#34;Didn&#39;t expect this many rounds.&#34;
        assert not self.expecting_observation, \
            &#34;Must observe rewards before choosing more arms&#34;
        
        self.expecting_observation = True

    @abstractmethod
    def observe_rewards(self, arms: List[int], rewards: List[float]) -&gt; None:
        &#34;&#34;&#34;Provide the bandit with the rewards for the arms just selected.

        Must only be called after calling `select_arms` to get the chosen arms.

        Parameters
        ----------
        arms : List[int]
            A list of the arms whose rewards are being provided. Should match
            the output of the last call to `select_arms()`.
        rewards : List[float]
            A list of the rewards at this round for the arms in `arms`, in the
            corresponding order. Rewards must be in the interval [0,1].
        &#34;&#34;&#34;
        assert self.expecting_observation, \
            &#34;Must choose arms before observing rewards&#34;
        assert len(arms) == len(rewards), &#34;Must have a reward for each arm&#34;
        assert all(0 &lt;= arm &lt; self.A for arm in arms), &#34;Invalid arms provided&#34;
        assert all(0 &lt;= reward &lt;= 1 for reward in rewards), \
            &#34;Rewards not in range [0, 1]&#34;

        self.expecting_observation = False
        self.round += 1


class AbstractFPML(AbstractMultiBandit):
    &#34;&#34;&#34;Abstract base class for the full-feedback parts of
    Follow the Perturbed Multiple Leaders.

    Extends `AbstractMultiBandit`.
    
    Parameters
    ----------
    Same as `AbstractMultiBandit`, plus:
    epsilon : float
        The rate of the i.i.d. exponential perturbations applied to cumulative
        rewards at each round.

    Attributes
    ----------
    Same as parameters.
    &#34;&#34;&#34;

    def __init__(self, A: int, T: int, n: int, epsilon: float):
        super().__init__(A, T, n) # The value of T here is meaningless
        self.epsilon = epsilon
        self._cum_est_rewards = np.zeros(A)

    def select_arms(self, _num: int, _store=True) -&gt; List[int]:
        &#34;&#34;&#34;Implements corresponding method in `AbstractMultiBandit`; but returns
        the top `_num` perturbed leaders, not `T`.
        &#34;&#34;&#34;
        if _store:
            super().select_arms()
        perturbations = np.random.exponential(scale=1/self.epsilon, size=self.A)
        perturbed_cum_est_rewards = self._cum_est_rewards + perturbations
        leaderboard = np.argsort(perturbed_cum_est_rewards)
        leaders = leaderboard[-_num:] if _num &gt; 0 else np.array([])
        return leaders.tolist()

    def _full_feedback_observation(self, rewards: List[float]) -&gt; None:
        assert len(rewards) == self.A, &#34;Must provide rewards for all arms&#34;
        self._cum_est_rewards += np.array(rewards)


class FPMLFixed(AbstractFPML):
    &#34;&#34;&#34;Implementation of the Follow the Perturbed Multiple Leaders algorithm
    with partial feedback and a fixed number of uniform exploration arms chosen
    at each round.

    Implements `AbstractMultiBandit`.

    Parameters
    ----------
    Same as `AbstractMultiBandit` plus:
    S : int, optional
        How many of the `T` arms at each round to set aside for exploration.
        Defaults to 1.

    Attributes
    ----------
    Same as parameters, plus:
    epsilon : float
        The rate of the i.i.d. exponential perturbations applied to cumulative
        rewards at each round.
    &#34;&#34;&#34;

    def __init__(self, A: int, T: int, n: int, S: int = 1):
        self.S = S
        super().__init__(A, T, n, epsilon=S/A*(np.log(A)/n)**(1/(T-S+1)))
        # Value of epsilon from Thm 3.9

    def select_arms(self) -&gt; List[int]:
        &#34;&#34;&#34;Implements corresponding method in `AbstractMultiBandit`.
        &#34;&#34;&#34;
        leaders = super().select_arms(_num=self.T-self.S)
        self._explore_arms = np.random.choice(np.arange(self.A),
                                              size=self.S,
                                              replace=False).tolist()
        return list(set(leaders+self._explore_arms))

    def observe_rewards(self, arms: List[int], rewards: List[float]) -&gt; None:
        &#34;&#34;&#34;Implements corresponding method in `AbstractMultiBandit`.
        &#34;&#34;&#34;
        super().observe_rewards(arms, rewards)
        estimates = np.zeros(self.A)
        for arm, reward in zip(arms, rewards):
            if arm in self._explore_arms:
                estimates[arm] = reward*self.A/self.S
        super()._full_feedback_observation(estimates.tolist())


class FPMLProb(AbstractFPML):
    &#34;&#34;&#34;Implementation of the Follow the Perturbed Multiple Leaders algorithm
    with partial feedback and probabilistic exploration.

    Implements `AbstractMultiBandit`.

    Parameters
    ----------
    Same as `AbstractMultiBandit`, plus:
    gamma : float
        The probability with which to replace individual chosen arms with
        a uniformly sampled exploration arm.

    Attributes
    ----------
    Same as parameters, plus:
    epsilon : float
        The rate of the i.i.d. exponential perturbations applied to cumulative
        rewards at each round.
    &#34;&#34;&#34;

    def __init__(self, A: int, T: int, n: int, gamma: float, epsilon: Optional[float] = None):
        self.gamma = gamma
        eps = gamma*T/A*(np.log(A)/n)**(1/(T-gamma*T+1)) if epsilon is None else epsilon
        super().__init__(A, T, n, epsilon=eps)
        # Value of epsilon adapted from Thm 3.9

    def select_arms(self) -&gt; List[int]:
        &#34;&#34;&#34;Implements corresponding method in `AbstractMultiBandit`.
        &#34;&#34;&#34;
        S = np.random.binomial(n=self.T, p=self.gamma)
        leaders = super().select_arms(_num=self.T-S)
        self._explore_arms = np.random.choice(np.arange(self.A),
                                              size=S,
                                              replace=False).tolist()
        return list(set(leaders+self._explore_arms))

    def observe_rewards(self, arms: List[int], rewards: List[float]) -&gt; None:
        &#34;&#34;&#34;Implements corresponding method in `AbstractMultiBandit`.
        &#34;&#34;&#34;
        super().observe_rewards(arms, rewards)
        estimates = np.zeros(self.A)
        for arm, reward in zip(arms, rewards):
            if arm in self._explore_arms:
                estimates[arm] = reward*self.A/(self.gamma*self.T)
        super()._full_feedback_observation(estimates.tolist())


class FPMLWithGR(AbstractFPML):
    &#34;&#34;&#34;Implementation of the Follow the Perturbed Multiple Leaders algorithm
    with partial feedback and geometric resampling for reward estimation.

    Implements `AbstractMultiBandit`.

    Parameters
    ----------
    Same as `AbstractMultiBandit`, plus:
    gamma : float, optional
        Float between 0 and 1 indicating the tendency to explore uniformly;
        defaults to 0, in which case no explicit exploration is performed.

    Attributes
    ----------
    Same as parameters, plus:
    epsilon : float
        The rate of the i.i.d. exponential perturbations applied to cumulative
        rewards at each round.
    &#34;&#34;&#34;

    def __init__(self, A: int, T: int, n: int,
                 gamma: float=0, epsilon: Optional[float] = None):
        self.gamma = gamma
        self._M = int(np.ceil(np.sqrt(A*(n/np.log(A))**(T/(T+1)))))
        eps = np.sqrt(1/A*(n/np.log(A))**((T-2)/(T+1))) \
              if epsilon is None else epsilon
        super().__init__(A, T, n, epsilon=eps)
        # Values from Prop 3.11
        # TODO: Update these for gamma&gt;0 case

    def select_arms(self, _store=True) -&gt; List[int]:
        &#34;&#34;&#34;Implements corresponding method in `AbstractMultiBandit`.
        &#34;&#34;&#34;
        S = np.random.binomial(n=self.T, p=self.gamma)
        leaders = super().select_arms(_num=self.T-S, _store=_store)
        explore = np.random.choice(np.arange(self.A), size=S, replace=False).tolist()
        selected = list(set(leaders+explore))
        if _store:
            self._selected = selected
        return selected

    def observe_rewards(self, arms: List[int], rewards: List[float]) -&gt; None:
        &#34;&#34;&#34;Implements corresponding method in `AbstractMultiBandit`.
        &#34;&#34;&#34;
        super().observe_rewards(arms, rewards)
        assert arms == self._selected, \
            &#34;Rewards must be provided for the chosen arms&#34;
        estimates = np.ones(self.A)
        estimates[arms] -= (1-np.array(rewards)) * self._geometric_resample(arms)
        super()._full_feedback_observation(estimates.tolist())

    def _geometric_resample(self, arms: List[int]) -&gt; np.array:
        geom = np.full(len(arms), self._M)
        for k in range(1, self._M):
            resampled_arms = self.select_arms(_store=False)
            for i, arm in enumerate(arms):
                if arm in resampled_arms:
                    geom[i] = min(geom[i], k)
        return geom


class StreeterFPML(AbstractMultiBandit):
    &#34;&#34;&#34;Implementation of the modified Streeter algorithm with multi-bandit
    FPML as a sub-routine.

    Implements `AbstractMultiBandit`.

    Parameters
    ----------
    Same as `AbstractMultiBandit` plus:
    T_1 : int
        The arm-budget for each internal instance of FPML.
    T_2 : int
        The number of internal instances of FPML.
    gamma : float
        The propensity to explore at each round, float between 0 and 1.
    gr : bool, optional
        Whether to use geometric resampling in the internal FPML instances.
        Defaults to False.

    Attributes
    ----------
    Same as parameters.

    Notes
    -----
    The parameters `T_1` and `T_2` must multiply to `T`.
    &#34;&#34;&#34;

    def __init__(self, A: int, T: int, T_1: int, T_2: int, n: int,
                 gamma: float, gr: bool = False, epsilon: Optional[float] = None):
        assert T_1 * T_2 == T, &#34;Time parameters must multiply to total budget&#34;
        self.T_1 = T_1
        self.T_2 = T_2
        self.gamma = gamma
        self.gr = gr
        fpml_class = FPMLWithGR if gr else FPMLProb
        self._internal_fpml_instances = [fpml_class(A, T_1, n, gamma, epsilon)
                                         for _ in range(T_2)]
        super().__init__(A, T, n)

    def select_arms(self) -&gt; List[int]:
        &#34;&#34;&#34;Implements corresponding method in `AbstractMultiBandit`.
        &#34;&#34;&#34;
        super().select_arms()
        self._grouped_arms = [self._internal_fpml_instances[t].select_arms()
                             for t in range(self.T_2)]
        return [arm for arms in self._grouped_arms for arm in arms]

    def observe_rewards(self, arms: List[int], rewards: List[float]) -&gt; None:
        &#34;&#34;&#34;Implements corresponding method in `AbstractMultiBandit`.
        &#34;&#34;&#34;
        super().observe_rewards(arms, rewards)
        assert arms == [arm for arms in self._grouped_arms for arm in arms], \
            &#34;Observed rewards must be for the chosen arms&#34;

        # Re-group rewards by internal FPML instance
        i = 0; grouped_rewards = []
        for arms in self._grouped_arms:
            grouped_rewards.append(rewards[i:i+len(arms)])
            i += len(arms)
        
        # Create modified &#39;greedy&#39; rewards
        max_rewards = [max(rewards) for rewards in grouped_rewards]
        grouped_modified_rewards = []
        for t, these_rewards in enumerate(grouped_rewards):
            cum_max = 0 if t == 0 else max(max_rewards[:t])
            modified_rewards = [max(reward, cum_max) - cum_max
                                for reward in these_rewards]
            grouped_modified_rewards.append(modified_rewards)

        # Feed back to internal FPML instances
        for t in range(self.T_2):
            self._internal_fpml_instances[t].observe_rewards(
                self._grouped_arms[t], grouped_modified_rewards[t]
            )
        
        self._grouped_arms = None


class Exp3(AbstractMultiBandit):
    &#34;&#34;&#34;Implementation of the standard Exp3 algorithm.

    Implements `AbstractMultiBandit`, but cannot recommend
    more than one arm per round.

    Parameters
    ----------
    A : int
        Number of arms available to choose from. Arms are subsequently referred
        to by their zero-indexed number.
    n : int
        Number of arms expected overall. Not actually used.
    gamma : Optional[float], optional
        Exploration parameter in [0,1] determining the probability of selecting
        an arm uniformly at each round. Chooses a good value if not provided.

    Attributes
    ----------
    Same as parameters.
    &#34;&#34;&#34;

    def __init__(self, A: int, n: int, gamma: Optional[float] = None):
        self.gamma = gamma if gamma is not None \
                           else np.sqrt(A*np.log(A))/(2/3*n*(np.e-1)) # Check this
        self._weights = np.ones(A)
        super().__init__(A=A, T=1, n=n)

    def select_arms(self) -&gt; List[int]:
        &#34;&#34;&#34;Implements the corresponding method from `AbstractMultiBandit`.

        Will always return a list of length 1.
        &#34;&#34;&#34;
        super().select_arms()
        prob_dist = (1-self.gamma) * self._weights / self._weights.sum() \
                + self.gamma / self.A
        self._arm = np.random.choice(np.arange(self.A), p=prob_dist)
        self._prob_chosen = prob_dist[self._arm]
        return [self._arm]

    def observe_rewards(self, arms: List[int], rewards: List[int]) -&gt; None:
        &#34;&#34;&#34;Implements the corresponding method from `AbstractMultiBandit`.

        The provided lists are required to be of length 1.
        &#34;&#34;&#34;
        super().observe_rewards(arms, rewards)
        assert len(arms) == 1 and len(rewards) == 1, \
            &#34;Only one arm and reward should be provided.&#34;
        assert arms[0] == self._arm, &#34;Arm doesn&#39;t match the selected arm.&#34;
        reward = rewards[0]
        est_reward = reward / self._prob_chosen
        self._weights[self._arm] *= np.exp(self.gamma * est_reward / self.A)


class Streeter(AbstractMultiBandit):
    &#34;&#34;&#34;Implementation of the standard Streeter online greedy algorithm
    (for unit-time actions) under partial feedback.
    
    Implements `AbstractMultiBandit`.

    Parameters
    ----------
    Same as parent class plus:
    gamma : Optional[float], optional
        Exploration probability for underlying Exp3 instances. Chooses a good
        value if not used.

    Attributes
    ----------
    Same as parameters.
    &#34;&#34;&#34;
    
    def __init__(self, A: int, T: int, n: int, gamma: Optional[float]=None):
        self.gamma = gamma
        self._internal_exp3_instances = [Exp3(A=A, n=n, gamma=gamma)
                                         for _ in range(T)]
        super().__init__(A, T, n)

    def select_arms(self) -&gt; List[int]:
        &#34;&#34;&#34;Implements the corresponding method from `AbstractMultiBandit`.
        &#34;&#34;&#34;
        super().select_arms()
        self._arms = [exp3.select_arms()[0]
                      for exp3 in self._internal_exp3_instances]
        return self._arms

    def observe_rewards(self, arms: List[int], rewards: List[int]) -&gt; None:
        &#34;&#34;&#34;Implements the corresponding method from `AbstractMultiBandit`.
        &#34;&#34;&#34;
        super().observe_rewards(arms, rewards)
        for t, exp3 in enumerate(self._internal_exp3_instances):
            r = rewards[0] if t == 0 else max(rewards[:(t+1)]) - max(rewards[:t])
            exp3.observe_rewards([arms[t]], [r])</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="blackboxbandits.bandits.AbstractFPML"><code class="flex name class">
<span>class <span class="ident">AbstractFPML</span></span>
<span>(</span><span>A: int, T: int, n: int, epsilon: float)</span>
</code></dt>
<dd>
<div class="desc"><p>Abstract base class for the full-feedback parts of
Follow the Perturbed Multiple Leaders.</p>
<p>Extends <code><a title="blackboxbandits.bandits.AbstractMultiBandit" href="#blackboxbandits.bandits.AbstractMultiBandit">AbstractMultiBandit</a></code>.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt>Same as <code><a title="blackboxbandits.bandits.AbstractMultiBandit" href="#blackboxbandits.bandits.AbstractMultiBandit">AbstractMultiBandit</a></code>, plus:</dt>
<dt><strong><code>epsilon</code></strong> :&ensp;<code>float</code></dt>
<dd>The rate of the i.i.d. exponential perturbations applied to cumulative
rewards at each round.</dd>
</dl>
<h2 id="attributes">Attributes</h2>
<p>Same as parameters.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class AbstractFPML(AbstractMultiBandit):
    &#34;&#34;&#34;Abstract base class for the full-feedback parts of
    Follow the Perturbed Multiple Leaders.

    Extends `AbstractMultiBandit`.
    
    Parameters
    ----------
    Same as `AbstractMultiBandit`, plus:
    epsilon : float
        The rate of the i.i.d. exponential perturbations applied to cumulative
        rewards at each round.

    Attributes
    ----------
    Same as parameters.
    &#34;&#34;&#34;

    def __init__(self, A: int, T: int, n: int, epsilon: float):
        super().__init__(A, T, n) # The value of T here is meaningless
        self.epsilon = epsilon
        self._cum_est_rewards = np.zeros(A)

    def select_arms(self, _num: int, _store=True) -&gt; List[int]:
        &#34;&#34;&#34;Implements corresponding method in `AbstractMultiBandit`; but returns
        the top `_num` perturbed leaders, not `T`.
        &#34;&#34;&#34;
        if _store:
            super().select_arms()
        perturbations = np.random.exponential(scale=1/self.epsilon, size=self.A)
        perturbed_cum_est_rewards = self._cum_est_rewards + perturbations
        leaderboard = np.argsort(perturbed_cum_est_rewards)
        leaders = leaderboard[-_num:] if _num &gt; 0 else np.array([])
        return leaders.tolist()

    def _full_feedback_observation(self, rewards: List[float]) -&gt; None:
        assert len(rewards) == self.A, &#34;Must provide rewards for all arms&#34;
        self._cum_est_rewards += np.array(rewards)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="blackboxbandits.bandits.AbstractMultiBandit" href="#blackboxbandits.bandits.AbstractMultiBandit">AbstractMultiBandit</a></li>
<li>abc.ABC</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="blackboxbandits.bandits.FPMLFixed" href="#blackboxbandits.bandits.FPMLFixed">FPMLFixed</a></li>
<li><a title="blackboxbandits.bandits.FPMLProb" href="#blackboxbandits.bandits.FPMLProb">FPMLProb</a></li>
<li><a title="blackboxbandits.bandits.FPMLWithGR" href="#blackboxbandits.bandits.FPMLWithGR">FPMLWithGR</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="blackboxbandits.bandits.AbstractFPML.select_arms"><code class="name flex">
<span>def <span class="ident">select_arms</span></span>(<span>self, _num: int) ‑> List[int]</span>
</code></dt>
<dd>
<div class="desc"><p>Implements corresponding method in <code><a title="blackboxbandits.bandits.AbstractMultiBandit" href="#blackboxbandits.bandits.AbstractMultiBandit">AbstractMultiBandit</a></code>; but returns
the top <code>_num</code> perturbed leaders, not <code>T</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def select_arms(self, _num: int, _store=True) -&gt; List[int]:
    &#34;&#34;&#34;Implements corresponding method in `AbstractMultiBandit`; but returns
    the top `_num` perturbed leaders, not `T`.
    &#34;&#34;&#34;
    if _store:
        super().select_arms()
    perturbations = np.random.exponential(scale=1/self.epsilon, size=self.A)
    perturbed_cum_est_rewards = self._cum_est_rewards + perturbations
    leaderboard = np.argsort(perturbed_cum_est_rewards)
    leaders = leaderboard[-_num:] if _num &gt; 0 else np.array([])
    return leaders.tolist()</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="blackboxbandits.bandits.AbstractMultiBandit" href="#blackboxbandits.bandits.AbstractMultiBandit">AbstractMultiBandit</a></b></code>:
<ul class="hlist">
<li><code><a title="blackboxbandits.bandits.AbstractMultiBandit.observe_rewards" href="#blackboxbandits.bandits.AbstractMultiBandit.observe_rewards">observe_rewards</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="blackboxbandits.bandits.AbstractMultiBandit"><code class="flex name class">
<span>class <span class="ident">AbstractMultiBandit</span></span>
<span>(</span><span>A: int, T: int, n: int)</span>
</code></dt>
<dd>
<div class="desc"><p>Abstract base class for multi-bandits.</p>
<p>Any instance will repeatedly choose a fixed number of arms to pull
and then observe the rewards for each of those arms, receiving the maximum
of those rewards.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>A</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of arms available to choose from. Arms are subsequently referred
to by their zero-indexed number.</dd>
<dt><strong><code>T</code></strong> :&ensp;<code>int</code></dt>
<dd>Arm budget at each round, i.e. how many arms the bandit is allowed to
pull.</dd>
<dt><strong><code>n</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of rounds expected overall. Used by some children.</dd>
</dl>
<h2 id="attributes">Attributes</h2>
<p>Same as parameters, plus:
self.round : int
The current round, between <code>0</code> and <code>n</code>.
self.expecting_observation : bool
Whether or not the bandit is currently waiting to receive the rewards
for the arms it just chose.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class AbstractMultiBandit(ABC):
    &#34;&#34;&#34;Abstract base class for multi-bandits.
    
    Any instance will repeatedly choose a fixed number of arms to pull
    and then observe the rewards for each of those arms, receiving the maximum
    of those rewards.
    
    Parameters
    ----------
    A : int
        Number of arms available to choose from. Arms are subsequently referred
        to by their zero-indexed number.
    T : int
        Arm budget at each round, i.e. how many arms the bandit is allowed to
        pull.
    n : int
        Number of rounds expected overall. Used by some children.

    Attributes
    ----------
    Same as parameters, plus:
    self.round : int
        The current round, between `0` and `n`.
    self.expecting_observation : bool
        Whether or not the bandit is currently waiting to receive the rewards
        for the arms it just chose.
    &#34;&#34;&#34;

    def __init__(self, A: int, T: int, n: int):
        self.A = A
        self.T = T
        self.n = n
        self.round = 0
        self.expecting_observation = False

    @abstractmethod
    def select_arms(self) -&gt; List[int]:
        &#34;&#34;&#34;Get arms selected by the bandit at the current round.

        Must only be called when the bandit isn&#39;t currently waiting for rewards.

        Returns
        -------
        List[int]
            A list of length `T` of the arms chosen by the bandit to pull at
            this round. Identified by their zero-indexed number.
        &#34;&#34;&#34;
        assert self.round &lt; self.n, \
            &#34;Didn&#39;t expect this many rounds.&#34;
        assert not self.expecting_observation, \
            &#34;Must observe rewards before choosing more arms&#34;
        
        self.expecting_observation = True

    @abstractmethod
    def observe_rewards(self, arms: List[int], rewards: List[float]) -&gt; None:
        &#34;&#34;&#34;Provide the bandit with the rewards for the arms just selected.

        Must only be called after calling `select_arms` to get the chosen arms.

        Parameters
        ----------
        arms : List[int]
            A list of the arms whose rewards are being provided. Should match
            the output of the last call to `select_arms()`.
        rewards : List[float]
            A list of the rewards at this round for the arms in `arms`, in the
            corresponding order. Rewards must be in the interval [0,1].
        &#34;&#34;&#34;
        assert self.expecting_observation, \
            &#34;Must choose arms before observing rewards&#34;
        assert len(arms) == len(rewards), &#34;Must have a reward for each arm&#34;
        assert all(0 &lt;= arm &lt; self.A for arm in arms), &#34;Invalid arms provided&#34;
        assert all(0 &lt;= reward &lt;= 1 for reward in rewards), \
            &#34;Rewards not in range [0, 1]&#34;

        self.expecting_observation = False
        self.round += 1</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>abc.ABC</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="blackboxbandits.bandits.AbstractFPML" href="#blackboxbandits.bandits.AbstractFPML">AbstractFPML</a></li>
<li><a title="blackboxbandits.bandits.Exp3" href="#blackboxbandits.bandits.Exp3">Exp3</a></li>
<li><a title="blackboxbandits.bandits.Streeter" href="#blackboxbandits.bandits.Streeter">Streeter</a></li>
<li><a title="blackboxbandits.bandits.StreeterFPML" href="#blackboxbandits.bandits.StreeterFPML">StreeterFPML</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="blackboxbandits.bandits.AbstractMultiBandit.observe_rewards"><code class="name flex">
<span>def <span class="ident">observe_rewards</span></span>(<span>self, arms: List[int], rewards: List[float]) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Provide the bandit with the rewards for the arms just selected.</p>
<p>Must only be called after calling <code>select_arms</code> to get the chosen arms.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>arms</code></strong> :&ensp;<code>List[int]</code></dt>
<dd>A list of the arms whose rewards are being provided. Should match
the output of the last call to <code>select_arms()</code>.</dd>
<dt><strong><code>rewards</code></strong> :&ensp;<code>List[float]</code></dt>
<dd>A list of the rewards at this round for the arms in <code>arms</code>, in the
corresponding order. Rewards must be in the interval [0,1].</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@abstractmethod
def observe_rewards(self, arms: List[int], rewards: List[float]) -&gt; None:
    &#34;&#34;&#34;Provide the bandit with the rewards for the arms just selected.

    Must only be called after calling `select_arms` to get the chosen arms.

    Parameters
    ----------
    arms : List[int]
        A list of the arms whose rewards are being provided. Should match
        the output of the last call to `select_arms()`.
    rewards : List[float]
        A list of the rewards at this round for the arms in `arms`, in the
        corresponding order. Rewards must be in the interval [0,1].
    &#34;&#34;&#34;
    assert self.expecting_observation, \
        &#34;Must choose arms before observing rewards&#34;
    assert len(arms) == len(rewards), &#34;Must have a reward for each arm&#34;
    assert all(0 &lt;= arm &lt; self.A for arm in arms), &#34;Invalid arms provided&#34;
    assert all(0 &lt;= reward &lt;= 1 for reward in rewards), \
        &#34;Rewards not in range [0, 1]&#34;

    self.expecting_observation = False
    self.round += 1</code></pre>
</details>
</dd>
<dt id="blackboxbandits.bandits.AbstractMultiBandit.select_arms"><code class="name flex">
<span>def <span class="ident">select_arms</span></span>(<span>self) ‑> List[int]</span>
</code></dt>
<dd>
<div class="desc"><p>Get arms selected by the bandit at the current round.</p>
<p>Must only be called when the bandit isn't currently waiting for rewards.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>List[int]</code></dt>
<dd>A list of length <code>T</code> of the arms chosen by the bandit to pull at
this round. Identified by their zero-indexed number.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@abstractmethod
def select_arms(self) -&gt; List[int]:
    &#34;&#34;&#34;Get arms selected by the bandit at the current round.

    Must only be called when the bandit isn&#39;t currently waiting for rewards.

    Returns
    -------
    List[int]
        A list of length `T` of the arms chosen by the bandit to pull at
        this round. Identified by their zero-indexed number.
    &#34;&#34;&#34;
    assert self.round &lt; self.n, \
        &#34;Didn&#39;t expect this many rounds.&#34;
    assert not self.expecting_observation, \
        &#34;Must observe rewards before choosing more arms&#34;
    
    self.expecting_observation = True</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="blackboxbandits.bandits.Exp3"><code class="flex name class">
<span>class <span class="ident">Exp3</span></span>
<span>(</span><span>A: int, n: int, gamma: Optional[float] = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Implementation of the standard Exp3 algorithm.</p>
<p>Implements <code><a title="blackboxbandits.bandits.AbstractMultiBandit" href="#blackboxbandits.bandits.AbstractMultiBandit">AbstractMultiBandit</a></code>, but cannot recommend
more than one arm per round.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>A</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of arms available to choose from. Arms are subsequently referred
to by their zero-indexed number.</dd>
<dt><strong><code>n</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of arms expected overall. Not actually used.</dd>
<dt><strong><code>gamma</code></strong> :&ensp;<code>Optional[float]</code>, optional</dt>
<dd>Exploration parameter in [0,1] determining the probability of selecting
an arm uniformly at each round. Chooses a good value if not provided.</dd>
</dl>
<h2 id="attributes">Attributes</h2>
<p>Same as parameters.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Exp3(AbstractMultiBandit):
    &#34;&#34;&#34;Implementation of the standard Exp3 algorithm.

    Implements `AbstractMultiBandit`, but cannot recommend
    more than one arm per round.

    Parameters
    ----------
    A : int
        Number of arms available to choose from. Arms are subsequently referred
        to by their zero-indexed number.
    n : int
        Number of arms expected overall. Not actually used.
    gamma : Optional[float], optional
        Exploration parameter in [0,1] determining the probability of selecting
        an arm uniformly at each round. Chooses a good value if not provided.

    Attributes
    ----------
    Same as parameters.
    &#34;&#34;&#34;

    def __init__(self, A: int, n: int, gamma: Optional[float] = None):
        self.gamma = gamma if gamma is not None \
                           else np.sqrt(A*np.log(A))/(2/3*n*(np.e-1)) # Check this
        self._weights = np.ones(A)
        super().__init__(A=A, T=1, n=n)

    def select_arms(self) -&gt; List[int]:
        &#34;&#34;&#34;Implements the corresponding method from `AbstractMultiBandit`.

        Will always return a list of length 1.
        &#34;&#34;&#34;
        super().select_arms()
        prob_dist = (1-self.gamma) * self._weights / self._weights.sum() \
                + self.gamma / self.A
        self._arm = np.random.choice(np.arange(self.A), p=prob_dist)
        self._prob_chosen = prob_dist[self._arm]
        return [self._arm]

    def observe_rewards(self, arms: List[int], rewards: List[int]) -&gt; None:
        &#34;&#34;&#34;Implements the corresponding method from `AbstractMultiBandit`.

        The provided lists are required to be of length 1.
        &#34;&#34;&#34;
        super().observe_rewards(arms, rewards)
        assert len(arms) == 1 and len(rewards) == 1, \
            &#34;Only one arm and reward should be provided.&#34;
        assert arms[0] == self._arm, &#34;Arm doesn&#39;t match the selected arm.&#34;
        reward = rewards[0]
        est_reward = reward / self._prob_chosen
        self._weights[self._arm] *= np.exp(self.gamma * est_reward / self.A)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="blackboxbandits.bandits.AbstractMultiBandit" href="#blackboxbandits.bandits.AbstractMultiBandit">AbstractMultiBandit</a></li>
<li>abc.ABC</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="blackboxbandits.bandits.Exp3.observe_rewards"><code class="name flex">
<span>def <span class="ident">observe_rewards</span></span>(<span>self, arms: List[int], rewards: List[int]) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Implements the corresponding method from <code><a title="blackboxbandits.bandits.AbstractMultiBandit" href="#blackboxbandits.bandits.AbstractMultiBandit">AbstractMultiBandit</a></code>.</p>
<p>The provided lists are required to be of length 1.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def observe_rewards(self, arms: List[int], rewards: List[int]) -&gt; None:
    &#34;&#34;&#34;Implements the corresponding method from `AbstractMultiBandit`.

    The provided lists are required to be of length 1.
    &#34;&#34;&#34;
    super().observe_rewards(arms, rewards)
    assert len(arms) == 1 and len(rewards) == 1, \
        &#34;Only one arm and reward should be provided.&#34;
    assert arms[0] == self._arm, &#34;Arm doesn&#39;t match the selected arm.&#34;
    reward = rewards[0]
    est_reward = reward / self._prob_chosen
    self._weights[self._arm] *= np.exp(self.gamma * est_reward / self.A)</code></pre>
</details>
</dd>
<dt id="blackboxbandits.bandits.Exp3.select_arms"><code class="name flex">
<span>def <span class="ident">select_arms</span></span>(<span>self) ‑> List[int]</span>
</code></dt>
<dd>
<div class="desc"><p>Implements the corresponding method from <code><a title="blackboxbandits.bandits.AbstractMultiBandit" href="#blackboxbandits.bandits.AbstractMultiBandit">AbstractMultiBandit</a></code>.</p>
<p>Will always return a list of length 1.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def select_arms(self) -&gt; List[int]:
    &#34;&#34;&#34;Implements the corresponding method from `AbstractMultiBandit`.

    Will always return a list of length 1.
    &#34;&#34;&#34;
    super().select_arms()
    prob_dist = (1-self.gamma) * self._weights / self._weights.sum() \
            + self.gamma / self.A
    self._arm = np.random.choice(np.arange(self.A), p=prob_dist)
    self._prob_chosen = prob_dist[self._arm]
    return [self._arm]</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="blackboxbandits.bandits.FPMLFixed"><code class="flex name class">
<span>class <span class="ident">FPMLFixed</span></span>
<span>(</span><span>A: int, T: int, n: int, S: int = 1)</span>
</code></dt>
<dd>
<div class="desc"><p>Implementation of the Follow the Perturbed Multiple Leaders algorithm
with partial feedback and a fixed number of uniform exploration arms chosen
at each round.</p>
<p>Implements <code><a title="blackboxbandits.bandits.AbstractMultiBandit" href="#blackboxbandits.bandits.AbstractMultiBandit">AbstractMultiBandit</a></code>.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt>Same as <code><a title="blackboxbandits.bandits.AbstractMultiBandit" href="#blackboxbandits.bandits.AbstractMultiBandit">AbstractMultiBandit</a></code> plus:</dt>
<dt><strong><code>S</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>How many of the <code>T</code> arms at each round to set aside for exploration.
Defaults to 1.</dd>
</dl>
<h2 id="attributes">Attributes</h2>
<dl>
<dt>Same as parameters, plus:</dt>
<dt><strong><code>epsilon</code></strong> :&ensp;<code>float</code></dt>
<dd>The rate of the i.i.d. exponential perturbations applied to cumulative
rewards at each round.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class FPMLFixed(AbstractFPML):
    &#34;&#34;&#34;Implementation of the Follow the Perturbed Multiple Leaders algorithm
    with partial feedback and a fixed number of uniform exploration arms chosen
    at each round.

    Implements `AbstractMultiBandit`.

    Parameters
    ----------
    Same as `AbstractMultiBandit` plus:
    S : int, optional
        How many of the `T` arms at each round to set aside for exploration.
        Defaults to 1.

    Attributes
    ----------
    Same as parameters, plus:
    epsilon : float
        The rate of the i.i.d. exponential perturbations applied to cumulative
        rewards at each round.
    &#34;&#34;&#34;

    def __init__(self, A: int, T: int, n: int, S: int = 1):
        self.S = S
        super().__init__(A, T, n, epsilon=S/A*(np.log(A)/n)**(1/(T-S+1)))
        # Value of epsilon from Thm 3.9

    def select_arms(self) -&gt; List[int]:
        &#34;&#34;&#34;Implements corresponding method in `AbstractMultiBandit`.
        &#34;&#34;&#34;
        leaders = super().select_arms(_num=self.T-self.S)
        self._explore_arms = np.random.choice(np.arange(self.A),
                                              size=self.S,
                                              replace=False).tolist()
        return list(set(leaders+self._explore_arms))

    def observe_rewards(self, arms: List[int], rewards: List[float]) -&gt; None:
        &#34;&#34;&#34;Implements corresponding method in `AbstractMultiBandit`.
        &#34;&#34;&#34;
        super().observe_rewards(arms, rewards)
        estimates = np.zeros(self.A)
        for arm, reward in zip(arms, rewards):
            if arm in self._explore_arms:
                estimates[arm] = reward*self.A/self.S
        super()._full_feedback_observation(estimates.tolist())</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="blackboxbandits.bandits.AbstractFPML" href="#blackboxbandits.bandits.AbstractFPML">AbstractFPML</a></li>
<li><a title="blackboxbandits.bandits.AbstractMultiBandit" href="#blackboxbandits.bandits.AbstractMultiBandit">AbstractMultiBandit</a></li>
<li>abc.ABC</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="blackboxbandits.bandits.FPMLFixed.observe_rewards"><code class="name flex">
<span>def <span class="ident">observe_rewards</span></span>(<span>self, arms: List[int], rewards: List[float]) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Implements corresponding method in <code><a title="blackboxbandits.bandits.AbstractMultiBandit" href="#blackboxbandits.bandits.AbstractMultiBandit">AbstractMultiBandit</a></code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def observe_rewards(self, arms: List[int], rewards: List[float]) -&gt; None:
    &#34;&#34;&#34;Implements corresponding method in `AbstractMultiBandit`.
    &#34;&#34;&#34;
    super().observe_rewards(arms, rewards)
    estimates = np.zeros(self.A)
    for arm, reward in zip(arms, rewards):
        if arm in self._explore_arms:
            estimates[arm] = reward*self.A/self.S
    super()._full_feedback_observation(estimates.tolist())</code></pre>
</details>
</dd>
<dt id="blackboxbandits.bandits.FPMLFixed.select_arms"><code class="name flex">
<span>def <span class="ident">select_arms</span></span>(<span>self) ‑> List[int]</span>
</code></dt>
<dd>
<div class="desc"><p>Implements corresponding method in <code><a title="blackboxbandits.bandits.AbstractMultiBandit" href="#blackboxbandits.bandits.AbstractMultiBandit">AbstractMultiBandit</a></code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def select_arms(self) -&gt; List[int]:
    &#34;&#34;&#34;Implements corresponding method in `AbstractMultiBandit`.
    &#34;&#34;&#34;
    leaders = super().select_arms(_num=self.T-self.S)
    self._explore_arms = np.random.choice(np.arange(self.A),
                                          size=self.S,
                                          replace=False).tolist()
    return list(set(leaders+self._explore_arms))</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="blackboxbandits.bandits.FPMLProb"><code class="flex name class">
<span>class <span class="ident">FPMLProb</span></span>
<span>(</span><span>A: int, T: int, n: int, gamma: float, epsilon: Optional[float] = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Implementation of the Follow the Perturbed Multiple Leaders algorithm
with partial feedback and probabilistic exploration.</p>
<p>Implements <code><a title="blackboxbandits.bandits.AbstractMultiBandit" href="#blackboxbandits.bandits.AbstractMultiBandit">AbstractMultiBandit</a></code>.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt>Same as <code><a title="blackboxbandits.bandits.AbstractMultiBandit" href="#blackboxbandits.bandits.AbstractMultiBandit">AbstractMultiBandit</a></code>, plus:</dt>
<dt><strong><code>gamma</code></strong> :&ensp;<code>float</code></dt>
<dd>The probability with which to replace individual chosen arms with
a uniformly sampled exploration arm.</dd>
</dl>
<h2 id="attributes">Attributes</h2>
<dl>
<dt>Same as parameters, plus:</dt>
<dt><strong><code>epsilon</code></strong> :&ensp;<code>float</code></dt>
<dd>The rate of the i.i.d. exponential perturbations applied to cumulative
rewards at each round.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class FPMLProb(AbstractFPML):
    &#34;&#34;&#34;Implementation of the Follow the Perturbed Multiple Leaders algorithm
    with partial feedback and probabilistic exploration.

    Implements `AbstractMultiBandit`.

    Parameters
    ----------
    Same as `AbstractMultiBandit`, plus:
    gamma : float
        The probability with which to replace individual chosen arms with
        a uniformly sampled exploration arm.

    Attributes
    ----------
    Same as parameters, plus:
    epsilon : float
        The rate of the i.i.d. exponential perturbations applied to cumulative
        rewards at each round.
    &#34;&#34;&#34;

    def __init__(self, A: int, T: int, n: int, gamma: float, epsilon: Optional[float] = None):
        self.gamma = gamma
        eps = gamma*T/A*(np.log(A)/n)**(1/(T-gamma*T+1)) if epsilon is None else epsilon
        super().__init__(A, T, n, epsilon=eps)
        # Value of epsilon adapted from Thm 3.9

    def select_arms(self) -&gt; List[int]:
        &#34;&#34;&#34;Implements corresponding method in `AbstractMultiBandit`.
        &#34;&#34;&#34;
        S = np.random.binomial(n=self.T, p=self.gamma)
        leaders = super().select_arms(_num=self.T-S)
        self._explore_arms = np.random.choice(np.arange(self.A),
                                              size=S,
                                              replace=False).tolist()
        return list(set(leaders+self._explore_arms))

    def observe_rewards(self, arms: List[int], rewards: List[float]) -&gt; None:
        &#34;&#34;&#34;Implements corresponding method in `AbstractMultiBandit`.
        &#34;&#34;&#34;
        super().observe_rewards(arms, rewards)
        estimates = np.zeros(self.A)
        for arm, reward in zip(arms, rewards):
            if arm in self._explore_arms:
                estimates[arm] = reward*self.A/(self.gamma*self.T)
        super()._full_feedback_observation(estimates.tolist())</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="blackboxbandits.bandits.AbstractFPML" href="#blackboxbandits.bandits.AbstractFPML">AbstractFPML</a></li>
<li><a title="blackboxbandits.bandits.AbstractMultiBandit" href="#blackboxbandits.bandits.AbstractMultiBandit">AbstractMultiBandit</a></li>
<li>abc.ABC</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="blackboxbandits.bandits.FPMLProb.observe_rewards"><code class="name flex">
<span>def <span class="ident">observe_rewards</span></span>(<span>self, arms: List[int], rewards: List[float]) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Implements corresponding method in <code><a title="blackboxbandits.bandits.AbstractMultiBandit" href="#blackboxbandits.bandits.AbstractMultiBandit">AbstractMultiBandit</a></code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def observe_rewards(self, arms: List[int], rewards: List[float]) -&gt; None:
    &#34;&#34;&#34;Implements corresponding method in `AbstractMultiBandit`.
    &#34;&#34;&#34;
    super().observe_rewards(arms, rewards)
    estimates = np.zeros(self.A)
    for arm, reward in zip(arms, rewards):
        if arm in self._explore_arms:
            estimates[arm] = reward*self.A/(self.gamma*self.T)
    super()._full_feedback_observation(estimates.tolist())</code></pre>
</details>
</dd>
<dt id="blackboxbandits.bandits.FPMLProb.select_arms"><code class="name flex">
<span>def <span class="ident">select_arms</span></span>(<span>self) ‑> List[int]</span>
</code></dt>
<dd>
<div class="desc"><p>Implements corresponding method in <code><a title="blackboxbandits.bandits.AbstractMultiBandit" href="#blackboxbandits.bandits.AbstractMultiBandit">AbstractMultiBandit</a></code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def select_arms(self) -&gt; List[int]:
    &#34;&#34;&#34;Implements corresponding method in `AbstractMultiBandit`.
    &#34;&#34;&#34;
    S = np.random.binomial(n=self.T, p=self.gamma)
    leaders = super().select_arms(_num=self.T-S)
    self._explore_arms = np.random.choice(np.arange(self.A),
                                          size=S,
                                          replace=False).tolist()
    return list(set(leaders+self._explore_arms))</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="blackboxbandits.bandits.FPMLWithGR"><code class="flex name class">
<span>class <span class="ident">FPMLWithGR</span></span>
<span>(</span><span>A: int, T: int, n: int, gamma: float = 0, epsilon: Optional[float] = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Implementation of the Follow the Perturbed Multiple Leaders algorithm
with partial feedback and geometric resampling for reward estimation.</p>
<p>Implements <code><a title="blackboxbandits.bandits.AbstractMultiBandit" href="#blackboxbandits.bandits.AbstractMultiBandit">AbstractMultiBandit</a></code>.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt>Same as <code><a title="blackboxbandits.bandits.AbstractMultiBandit" href="#blackboxbandits.bandits.AbstractMultiBandit">AbstractMultiBandit</a></code>, plus:</dt>
<dt><strong><code>gamma</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>Float between 0 and 1 indicating the tendency to explore uniformly;
defaults to 0, in which case no explicit exploration is performed.</dd>
</dl>
<h2 id="attributes">Attributes</h2>
<dl>
<dt>Same as parameters, plus:</dt>
<dt><strong><code>epsilon</code></strong> :&ensp;<code>float</code></dt>
<dd>The rate of the i.i.d. exponential perturbations applied to cumulative
rewards at each round.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class FPMLWithGR(AbstractFPML):
    &#34;&#34;&#34;Implementation of the Follow the Perturbed Multiple Leaders algorithm
    with partial feedback and geometric resampling for reward estimation.

    Implements `AbstractMultiBandit`.

    Parameters
    ----------
    Same as `AbstractMultiBandit`, plus:
    gamma : float, optional
        Float between 0 and 1 indicating the tendency to explore uniformly;
        defaults to 0, in which case no explicit exploration is performed.

    Attributes
    ----------
    Same as parameters, plus:
    epsilon : float
        The rate of the i.i.d. exponential perturbations applied to cumulative
        rewards at each round.
    &#34;&#34;&#34;

    def __init__(self, A: int, T: int, n: int,
                 gamma: float=0, epsilon: Optional[float] = None):
        self.gamma = gamma
        self._M = int(np.ceil(np.sqrt(A*(n/np.log(A))**(T/(T+1)))))
        eps = np.sqrt(1/A*(n/np.log(A))**((T-2)/(T+1))) \
              if epsilon is None else epsilon
        super().__init__(A, T, n, epsilon=eps)
        # Values from Prop 3.11
        # TODO: Update these for gamma&gt;0 case

    def select_arms(self, _store=True) -&gt; List[int]:
        &#34;&#34;&#34;Implements corresponding method in `AbstractMultiBandit`.
        &#34;&#34;&#34;
        S = np.random.binomial(n=self.T, p=self.gamma)
        leaders = super().select_arms(_num=self.T-S, _store=_store)
        explore = np.random.choice(np.arange(self.A), size=S, replace=False).tolist()
        selected = list(set(leaders+explore))
        if _store:
            self._selected = selected
        return selected

    def observe_rewards(self, arms: List[int], rewards: List[float]) -&gt; None:
        &#34;&#34;&#34;Implements corresponding method in `AbstractMultiBandit`.
        &#34;&#34;&#34;
        super().observe_rewards(arms, rewards)
        assert arms == self._selected, \
            &#34;Rewards must be provided for the chosen arms&#34;
        estimates = np.ones(self.A)
        estimates[arms] -= (1-np.array(rewards)) * self._geometric_resample(arms)
        super()._full_feedback_observation(estimates.tolist())

    def _geometric_resample(self, arms: List[int]) -&gt; np.array:
        geom = np.full(len(arms), self._M)
        for k in range(1, self._M):
            resampled_arms = self.select_arms(_store=False)
            for i, arm in enumerate(arms):
                if arm in resampled_arms:
                    geom[i] = min(geom[i], k)
        return geom</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="blackboxbandits.bandits.AbstractFPML" href="#blackboxbandits.bandits.AbstractFPML">AbstractFPML</a></li>
<li><a title="blackboxbandits.bandits.AbstractMultiBandit" href="#blackboxbandits.bandits.AbstractMultiBandit">AbstractMultiBandit</a></li>
<li>abc.ABC</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="blackboxbandits.bandits.FPMLWithGR.observe_rewards"><code class="name flex">
<span>def <span class="ident">observe_rewards</span></span>(<span>self, arms: List[int], rewards: List[float]) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Implements corresponding method in <code><a title="blackboxbandits.bandits.AbstractMultiBandit" href="#blackboxbandits.bandits.AbstractMultiBandit">AbstractMultiBandit</a></code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def observe_rewards(self, arms: List[int], rewards: List[float]) -&gt; None:
    &#34;&#34;&#34;Implements corresponding method in `AbstractMultiBandit`.
    &#34;&#34;&#34;
    super().observe_rewards(arms, rewards)
    assert arms == self._selected, \
        &#34;Rewards must be provided for the chosen arms&#34;
    estimates = np.ones(self.A)
    estimates[arms] -= (1-np.array(rewards)) * self._geometric_resample(arms)
    super()._full_feedback_observation(estimates.tolist())</code></pre>
</details>
</dd>
<dt id="blackboxbandits.bandits.FPMLWithGR.select_arms"><code class="name flex">
<span>def <span class="ident">select_arms</span></span>(<span>self) ‑> List[int]</span>
</code></dt>
<dd>
<div class="desc"><p>Implements corresponding method in <code><a title="blackboxbandits.bandits.AbstractMultiBandit" href="#blackboxbandits.bandits.AbstractMultiBandit">AbstractMultiBandit</a></code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def select_arms(self, _store=True) -&gt; List[int]:
    &#34;&#34;&#34;Implements corresponding method in `AbstractMultiBandit`.
    &#34;&#34;&#34;
    S = np.random.binomial(n=self.T, p=self.gamma)
    leaders = super().select_arms(_num=self.T-S, _store=_store)
    explore = np.random.choice(np.arange(self.A), size=S, replace=False).tolist()
    selected = list(set(leaders+explore))
    if _store:
        self._selected = selected
    return selected</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="blackboxbandits.bandits.Streeter"><code class="flex name class">
<span>class <span class="ident">Streeter</span></span>
<span>(</span><span>A: int, T: int, n: int, gamma: Optional[float] = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Implementation of the standard Streeter online greedy algorithm
(for unit-time actions) under partial feedback.</p>
<p>Implements <code><a title="blackboxbandits.bandits.AbstractMultiBandit" href="#blackboxbandits.bandits.AbstractMultiBandit">AbstractMultiBandit</a></code>.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt>Same as parent class plus:</dt>
<dt><strong><code>gamma</code></strong> :&ensp;<code>Optional[float]</code>, optional</dt>
<dd>Exploration probability for underlying Exp3 instances. Chooses a good
value if not used.</dd>
</dl>
<h2 id="attributes">Attributes</h2>
<p>Same as parameters.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Streeter(AbstractMultiBandit):
    &#34;&#34;&#34;Implementation of the standard Streeter online greedy algorithm
    (for unit-time actions) under partial feedback.
    
    Implements `AbstractMultiBandit`.

    Parameters
    ----------
    Same as parent class plus:
    gamma : Optional[float], optional
        Exploration probability for underlying Exp3 instances. Chooses a good
        value if not used.

    Attributes
    ----------
    Same as parameters.
    &#34;&#34;&#34;
    
    def __init__(self, A: int, T: int, n: int, gamma: Optional[float]=None):
        self.gamma = gamma
        self._internal_exp3_instances = [Exp3(A=A, n=n, gamma=gamma)
                                         for _ in range(T)]
        super().__init__(A, T, n)

    def select_arms(self) -&gt; List[int]:
        &#34;&#34;&#34;Implements the corresponding method from `AbstractMultiBandit`.
        &#34;&#34;&#34;
        super().select_arms()
        self._arms = [exp3.select_arms()[0]
                      for exp3 in self._internal_exp3_instances]
        return self._arms

    def observe_rewards(self, arms: List[int], rewards: List[int]) -&gt; None:
        &#34;&#34;&#34;Implements the corresponding method from `AbstractMultiBandit`.
        &#34;&#34;&#34;
        super().observe_rewards(arms, rewards)
        for t, exp3 in enumerate(self._internal_exp3_instances):
            r = rewards[0] if t == 0 else max(rewards[:(t+1)]) - max(rewards[:t])
            exp3.observe_rewards([arms[t]], [r])</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="blackboxbandits.bandits.AbstractMultiBandit" href="#blackboxbandits.bandits.AbstractMultiBandit">AbstractMultiBandit</a></li>
<li>abc.ABC</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="blackboxbandits.bandits.Streeter.observe_rewards"><code class="name flex">
<span>def <span class="ident">observe_rewards</span></span>(<span>self, arms: List[int], rewards: List[int]) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Implements the corresponding method from <code><a title="blackboxbandits.bandits.AbstractMultiBandit" href="#blackboxbandits.bandits.AbstractMultiBandit">AbstractMultiBandit</a></code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def observe_rewards(self, arms: List[int], rewards: List[int]) -&gt; None:
    &#34;&#34;&#34;Implements the corresponding method from `AbstractMultiBandit`.
    &#34;&#34;&#34;
    super().observe_rewards(arms, rewards)
    for t, exp3 in enumerate(self._internal_exp3_instances):
        r = rewards[0] if t == 0 else max(rewards[:(t+1)]) - max(rewards[:t])
        exp3.observe_rewards([arms[t]], [r])</code></pre>
</details>
</dd>
<dt id="blackboxbandits.bandits.Streeter.select_arms"><code class="name flex">
<span>def <span class="ident">select_arms</span></span>(<span>self) ‑> List[int]</span>
</code></dt>
<dd>
<div class="desc"><p>Implements the corresponding method from <code><a title="blackboxbandits.bandits.AbstractMultiBandit" href="#blackboxbandits.bandits.AbstractMultiBandit">AbstractMultiBandit</a></code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def select_arms(self) -&gt; List[int]:
    &#34;&#34;&#34;Implements the corresponding method from `AbstractMultiBandit`.
    &#34;&#34;&#34;
    super().select_arms()
    self._arms = [exp3.select_arms()[0]
                  for exp3 in self._internal_exp3_instances]
    return self._arms</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="blackboxbandits.bandits.StreeterFPML"><code class="flex name class">
<span>class <span class="ident">StreeterFPML</span></span>
<span>(</span><span>A: int, T: int, T_1: int, T_2: int, n: int, gamma: float, gr: bool = False, epsilon: Optional[float] = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Implementation of the modified Streeter algorithm with multi-bandit
FPML as a sub-routine.</p>
<p>Implements <code><a title="blackboxbandits.bandits.AbstractMultiBandit" href="#blackboxbandits.bandits.AbstractMultiBandit">AbstractMultiBandit</a></code>.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt>Same as <code><a title="blackboxbandits.bandits.AbstractMultiBandit" href="#blackboxbandits.bandits.AbstractMultiBandit">AbstractMultiBandit</a></code> plus:</dt>
<dt><strong><code>T_1</code></strong> :&ensp;<code>int</code></dt>
<dd>The arm-budget for each internal instance of FPML.</dd>
<dt><strong><code>T_2</code></strong> :&ensp;<code>int</code></dt>
<dd>The number of internal instances of FPML.</dd>
<dt><strong><code>gamma</code></strong> :&ensp;<code>float</code></dt>
<dd>The propensity to explore at each round, float between 0 and 1.</dd>
<dt><strong><code>gr</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to use geometric resampling in the internal FPML instances.
Defaults to False.</dd>
</dl>
<h2 id="attributes">Attributes</h2>
<p>Same as parameters.</p>
<h2 id="notes">Notes</h2>
<p>The parameters <code>T_1</code> and <code>T_2</code> must multiply to <code>T</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class StreeterFPML(AbstractMultiBandit):
    &#34;&#34;&#34;Implementation of the modified Streeter algorithm with multi-bandit
    FPML as a sub-routine.

    Implements `AbstractMultiBandit`.

    Parameters
    ----------
    Same as `AbstractMultiBandit` plus:
    T_1 : int
        The arm-budget for each internal instance of FPML.
    T_2 : int
        The number of internal instances of FPML.
    gamma : float
        The propensity to explore at each round, float between 0 and 1.
    gr : bool, optional
        Whether to use geometric resampling in the internal FPML instances.
        Defaults to False.

    Attributes
    ----------
    Same as parameters.

    Notes
    -----
    The parameters `T_1` and `T_2` must multiply to `T`.
    &#34;&#34;&#34;

    def __init__(self, A: int, T: int, T_1: int, T_2: int, n: int,
                 gamma: float, gr: bool = False, epsilon: Optional[float] = None):
        assert T_1 * T_2 == T, &#34;Time parameters must multiply to total budget&#34;
        self.T_1 = T_1
        self.T_2 = T_2
        self.gamma = gamma
        self.gr = gr
        fpml_class = FPMLWithGR if gr else FPMLProb
        self._internal_fpml_instances = [fpml_class(A, T_1, n, gamma, epsilon)
                                         for _ in range(T_2)]
        super().__init__(A, T, n)

    def select_arms(self) -&gt; List[int]:
        &#34;&#34;&#34;Implements corresponding method in `AbstractMultiBandit`.
        &#34;&#34;&#34;
        super().select_arms()
        self._grouped_arms = [self._internal_fpml_instances[t].select_arms()
                             for t in range(self.T_2)]
        return [arm for arms in self._grouped_arms for arm in arms]

    def observe_rewards(self, arms: List[int], rewards: List[float]) -&gt; None:
        &#34;&#34;&#34;Implements corresponding method in `AbstractMultiBandit`.
        &#34;&#34;&#34;
        super().observe_rewards(arms, rewards)
        assert arms == [arm for arms in self._grouped_arms for arm in arms], \
            &#34;Observed rewards must be for the chosen arms&#34;

        # Re-group rewards by internal FPML instance
        i = 0; grouped_rewards = []
        for arms in self._grouped_arms:
            grouped_rewards.append(rewards[i:i+len(arms)])
            i += len(arms)
        
        # Create modified &#39;greedy&#39; rewards
        max_rewards = [max(rewards) for rewards in grouped_rewards]
        grouped_modified_rewards = []
        for t, these_rewards in enumerate(grouped_rewards):
            cum_max = 0 if t == 0 else max(max_rewards[:t])
            modified_rewards = [max(reward, cum_max) - cum_max
                                for reward in these_rewards]
            grouped_modified_rewards.append(modified_rewards)

        # Feed back to internal FPML instances
        for t in range(self.T_2):
            self._internal_fpml_instances[t].observe_rewards(
                self._grouped_arms[t], grouped_modified_rewards[t]
            )
        
        self._grouped_arms = None</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="blackboxbandits.bandits.AbstractMultiBandit" href="#blackboxbandits.bandits.AbstractMultiBandit">AbstractMultiBandit</a></li>
<li>abc.ABC</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="blackboxbandits.bandits.StreeterFPML.observe_rewards"><code class="name flex">
<span>def <span class="ident">observe_rewards</span></span>(<span>self, arms: List[int], rewards: List[float]) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Implements corresponding method in <code><a title="blackboxbandits.bandits.AbstractMultiBandit" href="#blackboxbandits.bandits.AbstractMultiBandit">AbstractMultiBandit</a></code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def observe_rewards(self, arms: List[int], rewards: List[float]) -&gt; None:
    &#34;&#34;&#34;Implements corresponding method in `AbstractMultiBandit`.
    &#34;&#34;&#34;
    super().observe_rewards(arms, rewards)
    assert arms == [arm for arms in self._grouped_arms for arm in arms], \
        &#34;Observed rewards must be for the chosen arms&#34;

    # Re-group rewards by internal FPML instance
    i = 0; grouped_rewards = []
    for arms in self._grouped_arms:
        grouped_rewards.append(rewards[i:i+len(arms)])
        i += len(arms)
    
    # Create modified &#39;greedy&#39; rewards
    max_rewards = [max(rewards) for rewards in grouped_rewards]
    grouped_modified_rewards = []
    for t, these_rewards in enumerate(grouped_rewards):
        cum_max = 0 if t == 0 else max(max_rewards[:t])
        modified_rewards = [max(reward, cum_max) - cum_max
                            for reward in these_rewards]
        grouped_modified_rewards.append(modified_rewards)

    # Feed back to internal FPML instances
    for t in range(self.T_2):
        self._internal_fpml_instances[t].observe_rewards(
            self._grouped_arms[t], grouped_modified_rewards[t]
        )
    
    self._grouped_arms = None</code></pre>
</details>
</dd>
<dt id="blackboxbandits.bandits.StreeterFPML.select_arms"><code class="name flex">
<span>def <span class="ident">select_arms</span></span>(<span>self) ‑> List[int]</span>
</code></dt>
<dd>
<div class="desc"><p>Implements corresponding method in <code><a title="blackboxbandits.bandits.AbstractMultiBandit" href="#blackboxbandits.bandits.AbstractMultiBandit">AbstractMultiBandit</a></code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def select_arms(self) -&gt; List[int]:
    &#34;&#34;&#34;Implements corresponding method in `AbstractMultiBandit`.
    &#34;&#34;&#34;
    super().select_arms()
    self._grouped_arms = [self._internal_fpml_instances[t].select_arms()
                         for t in range(self.T_2)]
    return [arm for arms in self._grouped_arms for arm in arms]</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="blackboxbandits" href="index.html">blackboxbandits</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="blackboxbandits.bandits.AbstractFPML" href="#blackboxbandits.bandits.AbstractFPML">AbstractFPML</a></code></h4>
<ul class="">
<li><code><a title="blackboxbandits.bandits.AbstractFPML.select_arms" href="#blackboxbandits.bandits.AbstractFPML.select_arms">select_arms</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="blackboxbandits.bandits.AbstractMultiBandit" href="#blackboxbandits.bandits.AbstractMultiBandit">AbstractMultiBandit</a></code></h4>
<ul class="">
<li><code><a title="blackboxbandits.bandits.AbstractMultiBandit.observe_rewards" href="#blackboxbandits.bandits.AbstractMultiBandit.observe_rewards">observe_rewards</a></code></li>
<li><code><a title="blackboxbandits.bandits.AbstractMultiBandit.select_arms" href="#blackboxbandits.bandits.AbstractMultiBandit.select_arms">select_arms</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="blackboxbandits.bandits.Exp3" href="#blackboxbandits.bandits.Exp3">Exp3</a></code></h4>
<ul class="">
<li><code><a title="blackboxbandits.bandits.Exp3.observe_rewards" href="#blackboxbandits.bandits.Exp3.observe_rewards">observe_rewards</a></code></li>
<li><code><a title="blackboxbandits.bandits.Exp3.select_arms" href="#blackboxbandits.bandits.Exp3.select_arms">select_arms</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="blackboxbandits.bandits.FPMLFixed" href="#blackboxbandits.bandits.FPMLFixed">FPMLFixed</a></code></h4>
<ul class="">
<li><code><a title="blackboxbandits.bandits.FPMLFixed.observe_rewards" href="#blackboxbandits.bandits.FPMLFixed.observe_rewards">observe_rewards</a></code></li>
<li><code><a title="blackboxbandits.bandits.FPMLFixed.select_arms" href="#blackboxbandits.bandits.FPMLFixed.select_arms">select_arms</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="blackboxbandits.bandits.FPMLProb" href="#blackboxbandits.bandits.FPMLProb">FPMLProb</a></code></h4>
<ul class="">
<li><code><a title="blackboxbandits.bandits.FPMLProb.observe_rewards" href="#blackboxbandits.bandits.FPMLProb.observe_rewards">observe_rewards</a></code></li>
<li><code><a title="blackboxbandits.bandits.FPMLProb.select_arms" href="#blackboxbandits.bandits.FPMLProb.select_arms">select_arms</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="blackboxbandits.bandits.FPMLWithGR" href="#blackboxbandits.bandits.FPMLWithGR">FPMLWithGR</a></code></h4>
<ul class="">
<li><code><a title="blackboxbandits.bandits.FPMLWithGR.observe_rewards" href="#blackboxbandits.bandits.FPMLWithGR.observe_rewards">observe_rewards</a></code></li>
<li><code><a title="blackboxbandits.bandits.FPMLWithGR.select_arms" href="#blackboxbandits.bandits.FPMLWithGR.select_arms">select_arms</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="blackboxbandits.bandits.Streeter" href="#blackboxbandits.bandits.Streeter">Streeter</a></code></h4>
<ul class="">
<li><code><a title="blackboxbandits.bandits.Streeter.observe_rewards" href="#blackboxbandits.bandits.Streeter.observe_rewards">observe_rewards</a></code></li>
<li><code><a title="blackboxbandits.bandits.Streeter.select_arms" href="#blackboxbandits.bandits.Streeter.select_arms">select_arms</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="blackboxbandits.bandits.StreeterFPML" href="#blackboxbandits.bandits.StreeterFPML">StreeterFPML</a></code></h4>
<ul class="">
<li><code><a title="blackboxbandits.bandits.StreeterFPML.observe_rewards" href="#blackboxbandits.bandits.StreeterFPML.observe_rewards">observe_rewards</a></code></li>
<li><code><a title="blackboxbandits.bandits.StreeterFPML.select_arms" href="#blackboxbandits.bandits.StreeterFPML.select_arms">select_arms</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>