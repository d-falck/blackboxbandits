<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>blackboxbandits.compare API documentation</title>
<meta name="description" content="Provides interfaces to run experiments comparing black-box optimizers and
combinations of them (which we call meta-optimizers) on various ML tasks." />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>blackboxbandits.compare</code></h1>
</header>
<section id="section-intro">
<p>Provides interfaces to run experiments comparing black-box optimizers and
combinations of them (which we call meta-optimizers) on various ML tasks.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;Provides interfaces to run experiments comparing black-box optimizers and
combinations of them (which we call meta-optimizers) on various ML tasks.
&#34;&#34;&#34;

import pandas as pd
from .meta import AbstractMetaOptimizer
import os
import subprocess
from typing import List, Optional, Dict, Tuple
from bayesmark.serialize import XRSerializer
import numpy as np
from datetime import datetime
from tempfile import mkdtemp
from multiprocess import Pool, Lock
import tqdm
import time


class BaseOptimizerComparison:
    &#34;&#34;&#34;Interface for comparing standard black-box optimizers on ML tasks.

    Parameters
    ----------
    optimizers : List[str]
        List of standard optimizer names known to Bayesmark.
    classifiers : List[str]
        List of sklearn classification methods known to Bayesmark.
    datasets : List[str]
        List of classification/regression datasets known to Bayesmark.
    metrics : List[str]
        List of loss functions known to Bayesmark. Must include at least one
        regression and one classification loss function if `datasets` includes
        both types of task.
    num_calls : int
        Number of function evaluations allowed by each optimizer on each task.
    num_repetitions : int
        Number of times to repeat the entire experiment for reliability.
    db_root : str
        Path to root folder in which a folder for this experiment&#39;s data will
        be created.
    parallel : bool, optional
        Whether to run the experiments as a pool of tasks across multiple worker
        threads or not. Defaults to False.
    num_workers : Optional[int], optional
        Number of worker processes to use if parallelisation is enabled. If not
        specified defaults to the number of cpu cores.

    Attributes
    ----------
    Same as parameters.
    &#34;&#34;&#34;

    def __init__(self,
                 optimizers: List[str],
                 classifiers: List[str],
                 datasets: List[str],
                 metrics: List[str],
                 num_calls: int,
                 num_repetitions: int,
                 db_root: str,
                 parallel: bool = False,
                 num_workers: Optional[int] = None):
        
        self.optimizers = optimizers
        self.classifiers = classifiers
        self.datasets = datasets
        self.metrics = metrics
        self.num_calls = num_calls
        self.num_repetitions = num_repetitions
        self.db_root = db_root
        self.parallel = parallel
        self.num_workers = num_workers
        self._dbid: Optional[str] = None

    def run(self) -&gt; None:
        &#34;&#34;&#34;Run the comparison experiment defined by this class.

        May take a while, depending on whether parallelisation is enabled.

        Notes
        -----

        If parallelisation is enabled, this generates and uses a temporary
        file `jobs.txt` in the current directory.
        &#34;&#34;&#34;
        launcher_args = {
            &#34;-dir&#34;: self.db_root,
            &#34;-o&#34;: &#34; &#34;.join(self.optimizers),
            &#34;-d&#34;: &#34; &#34;.join(self.datasets),
            &#34;-c&#34;: &#34; &#34;.join(self.classifiers),
            &#34;-m&#34;: &#34; &#34;.join(self.metrics),
            &#34;-n&#34;: str(self.num_calls),
            &#34;-r&#34;: str(self.num_repetitions)
        }
        if self.parallel: # Will create list of independent commands to run
            # Approximate number of indep. experiments in this comparison
            launcher_args[&#34;-nj&#34;] = str(len(self.optimizers) * len(self.datasets) \
                                 * len(self.classifiers) * self.num_repetitions)

            # Generate dbid for whole batch
            folder_prefix = datetime.utcnow().strftime(&#34;bo_%Y%m%d_%H%M%S_&#34;)
            exp_subdir = mkdtemp(prefix=folder_prefix, dir=self.db_root)
            self._dbid = os.path.basename(exp_subdir)
            launcher_args[&#34;-b&#34;] = self._dbid

            # Setup dbid folder
            for name in [&#34;derived&#34;, &#34;log&#34;, &#34;eval&#34;, &#34;time&#34;, &#34;suggest_log&#34;]:
                os.mkdir(os.path.join(exp_subdir, name))

        # Run Bayesmark experiment launcher
        launcher_command = &#34;bayesmark-launch &#34; + &#34; &#34;.join(
            [key + &#34; &#34; + value for key, value in launcher_args.items()]
        )
        launcher_command += &#34; -v&#34;
        p = subprocess.Popen(launcher_command,
                             stderr=subprocess.PIPE, shell=True)
        while p.poll() is None:
            l = p.stderr.readline().decode(&#34;utf-8&#34;)
            print(l)
            if &#34;Supply --db&#34; in l:
                dbid_string = l

        # If parallel we now need to run the generated commands
        if self.parallel:
            self._run_parallel_commands(self._dbid)
        else:
            # Get DBID
            dbid_start_index = dbid_string.find(&#34;Supply --db&#34;) \
                                + len(&#34;Supply --db&#34;) + 1
            dbid_end_index = dbid_string.find(&#34;to append to this experiment&#34;) - 1
            assert dbid_start_index != -1 and dbid_end_index != -1
            self._dbid = dbid_string[dbid_start_index:dbid_end_index]

        # Aggregate results
        os.system(f&#39;bayesmark-agg -dir &#34;{self.db_root}&#34; -b &#34;{self._dbid}&#34;&#39;)

        # Analyse results (will compute baselines which we&#39;ll use)
        os.system(f&#39;bayesmark-anal -dir &#34;{self.db_root}&#34; -b &#34;{self._dbid}&#34;&#39;)

    def get_dbid(self) -&gt; int:
        &#34;&#34;&#34;Get the unique DBID of this comparison.
        
        This is the name of the folder this comparison&#39;s data is saved in.
        Must run comparison first.

        Returns
        -------
        int
            The DBID of this experiment.
        &#34;&#34;&#34;
        assert self._dbid is not None, &#34;Must run comparison first.&#34;
        return self._dbid

    def get_results(self) -&gt; pd.DataFrame:
        &#34;&#34;&#34;Get the results of this comparison as a dataframe.

        Returns
        -------
        pd.DataFrame
            A dataframe containing the results of this comparison.
        &#34;&#34;&#34;
        return self.get_results_for_dbid(self._dbid, self.db_root)

    def _run_parallel_commands(self, dbid: int) -&gt; None:
        with open(&#34;./jobs.txt&#34;,&#34;r&#34;) as f:
            jobs = f.readlines()[2:]
        cmd_start_idx = jobs[0].find(&#34;bayesmark-exp&#34;)
        job_commands = {job[:cmd_start_idx-1]: job[cmd_start_idx:].strip()
                        for job in jobs}
        
        l = Lock() # We&#39;ll make `lock&#39; global among pooled processes
        pool = Pool(self.num_workers, initializer=self._pool_init, initargs=(l,)) \
               if self.num_workers is not None \
               else Pool(initializer=self._pool_init, initargs=(l,))
        print(f&#34;Starting processing {len(job_commands)} jobs.&#34;)
        pool.map(self._process_individual_command, job_commands.items())
        pool.close()
        pool.join()
        print(&#34;Finished processing all jobs.&#34;)

    def _process_individual_command(self, job: Tuple[str, str]):
        with lock:
            print(&#34;Starting job&#34;, job[0])
        try:
            subprocess.run(job[1], shell=True)
        except:
            with lock:
                print(f&#34;Job {job[0]} failed&#34;)
        else:
            with lock:
                print(&#34;Finished job&#34;, job[0])
            
    def _pool_init(self, l):
        global lock
        lock = l

    @classmethod
    def get_results_for_dbid(cls, dbid: str, db_root: str) -&gt; pd.DataFrame:
        &#34;&#34;&#34;Get the results of a particular comparison specified by its DBID.

        Parameters
        ----------
        dbid : str
            The DBID of the comparison to get results for.
        db_root : str
            Path to the directory containing the folder for
            the relevant comparison.
        
        Returns
        -------
        pd.DataFrame
            A dataframe containing the result of the specified comparison.
        &#34;&#34;&#34;
        # Read data saved by Bayesmark
        abs_db_root = os.path.abspath(db_root)
        saved_eval = XRSerializer.load_derived(db_root=abs_db_root,
                                               db=dbid,
                                               key=&#34;eval&#34;)
        eval = saved_eval[0].to_dataframe()
        saved_baseline = XRSerializer.load_derived(db_root=abs_db_root,
                                                   db=dbid,
                                                   key=&#34;baseline&#34;)
        baseline = saved_baseline[0].to_dataframe()

        # Massage into nice format
        baseline = baseline[[&#34;mean&#34;,&#34;best&#34;]] \
            .groupby(level=[&#34;objective&#34;,&#34;function&#34;]).min()
        baseline = baseline.unstack(level=&#34;objective&#34;)
        baseline.columns = [&#34;visible_baseline&#34;, &#34;generalization_baseline&#34;,
                            &#34;visible_opt&#34;, &#34;generalization_opt&#34;]
        eval = eval.droplevel(&#34;suggestion&#34;)
        eval = eval.reorder_levels([&#34;optimizer&#34;, &#34;function&#34;,
                                    &#34;study_id&#34;, &#34;iter&#34;])
        eval = eval.sort_values(eval.index.names)
        eval = eval.groupby(level=[&#34;optimizer&#34;,&#34;function&#34;,&#34;study_id&#34;]).min()
        eval = eval.rename(columns={&#34;_visible_to_opt&#34;: &#34;visible&#34;})
        eval = eval.rename(columns=lambda x: x + &#34;_achieved&#34;)
        data = eval.join(baseline, on=&#34;function&#34;)

        # Calculate normalized scores
        data[&#34;generalization_score&#34;] = cls._constrain(
            1 - (data[&#34;generalization_achieved&#34;] - data[&#34;generalization_opt&#34;]) \
            / (data[&#34;generalization_baseline&#34;] - data[&#34;generalization_opt&#34;])
        )
        data[&#34;visible_score&#34;] = cls._constrain(
            1 - (data[&#34;visible_achieved&#34;] - data[&#34;visible_opt&#34;]) \
            / (data[&#34;visible_baseline&#34;] - data[&#34;visible_opt&#34;])
        )

        data = data.reindex(sorted(data.columns), axis=1)
        data.columns = data.columns.str.split(&#34;_&#34;, expand=True)

        return data

    @staticmethod
    def _constrain(series):
        return np.maximum(np.minimum(series, 1), 0)


class MetaOptimizerComparison:
    &#34;&#34;&#34;Interface for comparing *combinations* of standard black-box
    optimizers on ML tasks.

    Parameters
    ----------
    meta_optimizers : Dict[str, meta.AbstractMetaOptimizer]
        Dictionary whose values are the instantiated meta-optimizers to include
        in this comparison (these will choose combinations of the base optimizers
        defined below) and whose keys are the names used to refer to them.
    base_optimizers : List[str]
        List of standard optimizer names known to Bayesmark to be used as the
        base optimizers for this comparison.
    classifiers : List[str]
        List of sklearn classification methods known to Bayesmark.
    datasets : List[str]
        List of classification/regression datasets known to Bayesmark.
    metrics : List[str]
        List of loss functions known to Bayesmark. Must include at least one
        regression and one classification loss function if `datasets` includes
        both types of task.
    num_calls : int
        Number of function evaluations allowed by each optimizer on each task.
    num_repetitions : int
        Number of times to repeat the entire experiment for reliability.
    db_root : str
        Path to root folder in which a folder for this experiment&#39;s data will
        be created.
    parallel : bool, optional
        Whether to run the base experiments as a pool of tasks across multiple
        worker threads or not. Defaults to False.
    num_workers : Optional[int], optional
        Number of worker processes to use if parallelisation is enabled. If not
        specified defaults to the number of cpu cores.

    Attributes
    ----------
    Same as parameters.
    &#34;&#34;&#34;

    def __init__(self,
                 meta_optimizers: Dict[str, AbstractMetaOptimizer],
                 base_optimizers: List[str],
                 classifiers: List[str],
                 datasets: List[str],
                 metrics: List[str],
                 num_calls: int,
                 num_repetitions: int,
                 db_root: str,
                 parallel: bool = False,
                 num_workers: Optional[int] = None):
        self.meta_optimizers = meta_optimizers
        self.base_optimizers = base_optimizers
        self.classifiers = classifiers
        self.datasets = datasets
        self.metrics = metrics
        self.num_calls = num_calls
        self.num_repetitions = num_repetitions
        self.db_root = db_root
        self.parallel = parallel
        self.num_workers = num_workers
        
        self._dbid: Optional[str] = None

        self._meta_comparison_completed = False

    def run_base_comparison(self) -&gt; None:
        &#34;&#34;&#34;Run the base optimizers on the relevant tasks for this experiment.

        May take a while.
        &#34;&#34;&#34;
        base_comparison = BaseOptimizerComparison(
            self.base_optimizers,
            self.classifiers,
            self.datasets,
            self.metrics,
            self.num_calls,
            self.num_repetitions,
            self.db_root,
            self.parallel,
            self.num_workers)
        base_comparison.run()
        self._dbid = base_comparison.get_dbid()
        self._base_comparison_data = base_comparison.get_results()

    def load_base_comparison(self, dbid: str) -&gt; None:
        &#34;&#34;&#34;Load data from a previously run comparison of the base optimizers
        for this experiment.

        The DBID must correspond to a comparison of the correct base optimizers
        for this meta_comparison.

        Parameters
        ----------
        dbid : str
            The DBID for the saved data to be loaded.
        &#34;&#34;&#34;
        self._dbid = dbid
        self._base_comparison_data = BaseOptimizerComparison \
            .get_results_for_dbid(self._dbid, self.db_root)

    def run_meta_comparison(self):
        &#34;&#34;&#34;Run the comparison of meta-optimizers over the base optimizers.

        Must have first run or loaded the base optimizer comparison.
        &#34;&#34;&#34;
        assert self._dbid is not None, &#34;Must run or load base comparison first.&#34;
        self._meta_comparison_completed = True

        all_results = []
        for rep in range(self.num_repetitions):
            results = []
            for meta_optimizer in self.meta_optimizers.values():
                comp_data = self._base_comparison_data.xs(rep, level=&#34;study_id&#34;)
                meta_optimizer.run(comp_data)
                results.append(meta_optimizer.get_results())
            all_results.append(pd.concat(results,
                                         keys=self.meta_optimizers.keys()))
        self._all_meta_results = pd.concat(
            all_results,
            keys=list(range(self.num_repetitions))
        )
        self._all_meta_results.index.rename(
            [&#34;study_id&#34;, &#34;optimizer&#34;, &#34;function&#34;],
            inplace=True
        )
        self._all_meta_results = self._all_meta_results \
            .reorder_levels([&#34;optimizer&#34;, &#34;function&#34;, &#34;study_id&#34;])
        self._all_meta_results = self._all_meta_results \
                                .sort_values(self._all_meta_results.index.names)
    def full_results(self) -&gt; pd.DataFrame:
        &#34;&#34;&#34;Get the full results of this meta-comparison as a dataframe.
        
        Must have run the meta-comparison first.
        
        Returns
        -------
        pd.DataFrame
            A dataframe containing the average visible and generalization scores
            for each base and each meta optimizer, on each task.
        &#34;&#34;&#34;
        assert self._meta_comparison_completed == True, \
             &#34;Must complete comparison before getting results&#34;
        base_results = self._base_comparison_data.xs(&#34;score&#34;, level=1, axis=1) \
                                        .rename(columns=lambda x: x+&#34;_score&#34;)
        return pd.concat([base_results, self._all_meta_results]) \
                .groupby([&#34;optimizer&#34;, &#34;function&#34;]).mean() \
                    [[&#34;visible_score&#34;, &#34;generalization_score&#34;]]

    def summary(self) -&gt; pd.DataFrame:
        &#34;&#34;&#34;Get summarized results of this meta-comparison as a dataframe.
        
        Must have run the meta-comparison first.
        
        Returns
        -------
        pd.DataFrame
            A dataframe containing visible and generalization scores
            for each base and each meta optimizer averaged over all tasks.
        &#34;&#34;&#34;
        return self.full_results().groupby([&#34;optimizer&#34;]).mean()

    def get_dbid(self):
        &#34;&#34;&#34;Get the unique DBID associated with the base optimizer comparison
        for this experiment.

        Must run or load base comparison first.

        Returns
        -------
        int
            The DBID of this base comparison for this experiment.
        &#34;&#34;&#34;
        assert self._dbid is not None, &#34;Must run or load base comparison first.&#34;
        return self._dbid</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="blackboxbandits.compare.BaseOptimizerComparison"><code class="flex name class">
<span>class <span class="ident">BaseOptimizerComparison</span></span>
<span>(</span><span>optimizers: List[str], classifiers: List[str], datasets: List[str], metrics: List[str], num_calls: int, num_repetitions: int, db_root: str, parallel: bool = False, num_workers: Optional[int] = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Interface for comparing standard black-box optimizers on ML tasks.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>optimizers</code></strong> :&ensp;<code>List[str]</code></dt>
<dd>List of standard optimizer names known to Bayesmark.</dd>
<dt><strong><code>classifiers</code></strong> :&ensp;<code>List[str]</code></dt>
<dd>List of sklearn classification methods known to Bayesmark.</dd>
<dt><strong><code>datasets</code></strong> :&ensp;<code>List[str]</code></dt>
<dd>List of classification/regression datasets known to Bayesmark.</dd>
<dt><strong><code>metrics</code></strong> :&ensp;<code>List[str]</code></dt>
<dd>List of loss functions known to Bayesmark. Must include at least one
regression and one classification loss function if <code>datasets</code> includes
both types of task.</dd>
<dt><strong><code>num_calls</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of function evaluations allowed by each optimizer on each task.</dd>
<dt><strong><code>num_repetitions</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of times to repeat the entire experiment for reliability.</dd>
<dt><strong><code>db_root</code></strong> :&ensp;<code>str</code></dt>
<dd>Path to root folder in which a folder for this experiment's data will
be created.</dd>
<dt><strong><code>parallel</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to run the experiments as a pool of tasks across multiple worker
threads or not. Defaults to False.</dd>
<dt><strong><code>num_workers</code></strong> :&ensp;<code>Optional[int]</code>, optional</dt>
<dd>Number of worker processes to use if parallelisation is enabled. If not
specified defaults to the number of cpu cores.</dd>
</dl>
<h2 id="attributes">Attributes</h2>
<p>Same as parameters.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class BaseOptimizerComparison:
    &#34;&#34;&#34;Interface for comparing standard black-box optimizers on ML tasks.

    Parameters
    ----------
    optimizers : List[str]
        List of standard optimizer names known to Bayesmark.
    classifiers : List[str]
        List of sklearn classification methods known to Bayesmark.
    datasets : List[str]
        List of classification/regression datasets known to Bayesmark.
    metrics : List[str]
        List of loss functions known to Bayesmark. Must include at least one
        regression and one classification loss function if `datasets` includes
        both types of task.
    num_calls : int
        Number of function evaluations allowed by each optimizer on each task.
    num_repetitions : int
        Number of times to repeat the entire experiment for reliability.
    db_root : str
        Path to root folder in which a folder for this experiment&#39;s data will
        be created.
    parallel : bool, optional
        Whether to run the experiments as a pool of tasks across multiple worker
        threads or not. Defaults to False.
    num_workers : Optional[int], optional
        Number of worker processes to use if parallelisation is enabled. If not
        specified defaults to the number of cpu cores.

    Attributes
    ----------
    Same as parameters.
    &#34;&#34;&#34;

    def __init__(self,
                 optimizers: List[str],
                 classifiers: List[str],
                 datasets: List[str],
                 metrics: List[str],
                 num_calls: int,
                 num_repetitions: int,
                 db_root: str,
                 parallel: bool = False,
                 num_workers: Optional[int] = None):
        
        self.optimizers = optimizers
        self.classifiers = classifiers
        self.datasets = datasets
        self.metrics = metrics
        self.num_calls = num_calls
        self.num_repetitions = num_repetitions
        self.db_root = db_root
        self.parallel = parallel
        self.num_workers = num_workers
        self._dbid: Optional[str] = None

    def run(self) -&gt; None:
        &#34;&#34;&#34;Run the comparison experiment defined by this class.

        May take a while, depending on whether parallelisation is enabled.

        Notes
        -----

        If parallelisation is enabled, this generates and uses a temporary
        file `jobs.txt` in the current directory.
        &#34;&#34;&#34;
        launcher_args = {
            &#34;-dir&#34;: self.db_root,
            &#34;-o&#34;: &#34; &#34;.join(self.optimizers),
            &#34;-d&#34;: &#34; &#34;.join(self.datasets),
            &#34;-c&#34;: &#34; &#34;.join(self.classifiers),
            &#34;-m&#34;: &#34; &#34;.join(self.metrics),
            &#34;-n&#34;: str(self.num_calls),
            &#34;-r&#34;: str(self.num_repetitions)
        }
        if self.parallel: # Will create list of independent commands to run
            # Approximate number of indep. experiments in this comparison
            launcher_args[&#34;-nj&#34;] = str(len(self.optimizers) * len(self.datasets) \
                                 * len(self.classifiers) * self.num_repetitions)

            # Generate dbid for whole batch
            folder_prefix = datetime.utcnow().strftime(&#34;bo_%Y%m%d_%H%M%S_&#34;)
            exp_subdir = mkdtemp(prefix=folder_prefix, dir=self.db_root)
            self._dbid = os.path.basename(exp_subdir)
            launcher_args[&#34;-b&#34;] = self._dbid

            # Setup dbid folder
            for name in [&#34;derived&#34;, &#34;log&#34;, &#34;eval&#34;, &#34;time&#34;, &#34;suggest_log&#34;]:
                os.mkdir(os.path.join(exp_subdir, name))

        # Run Bayesmark experiment launcher
        launcher_command = &#34;bayesmark-launch &#34; + &#34; &#34;.join(
            [key + &#34; &#34; + value for key, value in launcher_args.items()]
        )
        launcher_command += &#34; -v&#34;
        p = subprocess.Popen(launcher_command,
                             stderr=subprocess.PIPE, shell=True)
        while p.poll() is None:
            l = p.stderr.readline().decode(&#34;utf-8&#34;)
            print(l)
            if &#34;Supply --db&#34; in l:
                dbid_string = l

        # If parallel we now need to run the generated commands
        if self.parallel:
            self._run_parallel_commands(self._dbid)
        else:
            # Get DBID
            dbid_start_index = dbid_string.find(&#34;Supply --db&#34;) \
                                + len(&#34;Supply --db&#34;) + 1
            dbid_end_index = dbid_string.find(&#34;to append to this experiment&#34;) - 1
            assert dbid_start_index != -1 and dbid_end_index != -1
            self._dbid = dbid_string[dbid_start_index:dbid_end_index]

        # Aggregate results
        os.system(f&#39;bayesmark-agg -dir &#34;{self.db_root}&#34; -b &#34;{self._dbid}&#34;&#39;)

        # Analyse results (will compute baselines which we&#39;ll use)
        os.system(f&#39;bayesmark-anal -dir &#34;{self.db_root}&#34; -b &#34;{self._dbid}&#34;&#39;)

    def get_dbid(self) -&gt; int:
        &#34;&#34;&#34;Get the unique DBID of this comparison.
        
        This is the name of the folder this comparison&#39;s data is saved in.
        Must run comparison first.

        Returns
        -------
        int
            The DBID of this experiment.
        &#34;&#34;&#34;
        assert self._dbid is not None, &#34;Must run comparison first.&#34;
        return self._dbid

    def get_results(self) -&gt; pd.DataFrame:
        &#34;&#34;&#34;Get the results of this comparison as a dataframe.

        Returns
        -------
        pd.DataFrame
            A dataframe containing the results of this comparison.
        &#34;&#34;&#34;
        return self.get_results_for_dbid(self._dbid, self.db_root)

    def _run_parallel_commands(self, dbid: int) -&gt; None:
        with open(&#34;./jobs.txt&#34;,&#34;r&#34;) as f:
            jobs = f.readlines()[2:]
        cmd_start_idx = jobs[0].find(&#34;bayesmark-exp&#34;)
        job_commands = {job[:cmd_start_idx-1]: job[cmd_start_idx:].strip()
                        for job in jobs}
        
        l = Lock() # We&#39;ll make `lock&#39; global among pooled processes
        pool = Pool(self.num_workers, initializer=self._pool_init, initargs=(l,)) \
               if self.num_workers is not None \
               else Pool(initializer=self._pool_init, initargs=(l,))
        print(f&#34;Starting processing {len(job_commands)} jobs.&#34;)
        pool.map(self._process_individual_command, job_commands.items())
        pool.close()
        pool.join()
        print(&#34;Finished processing all jobs.&#34;)

    def _process_individual_command(self, job: Tuple[str, str]):
        with lock:
            print(&#34;Starting job&#34;, job[0])
        try:
            subprocess.run(job[1], shell=True)
        except:
            with lock:
                print(f&#34;Job {job[0]} failed&#34;)
        else:
            with lock:
                print(&#34;Finished job&#34;, job[0])
            
    def _pool_init(self, l):
        global lock
        lock = l

    @classmethod
    def get_results_for_dbid(cls, dbid: str, db_root: str) -&gt; pd.DataFrame:
        &#34;&#34;&#34;Get the results of a particular comparison specified by its DBID.

        Parameters
        ----------
        dbid : str
            The DBID of the comparison to get results for.
        db_root : str
            Path to the directory containing the folder for
            the relevant comparison.
        
        Returns
        -------
        pd.DataFrame
            A dataframe containing the result of the specified comparison.
        &#34;&#34;&#34;
        # Read data saved by Bayesmark
        abs_db_root = os.path.abspath(db_root)
        saved_eval = XRSerializer.load_derived(db_root=abs_db_root,
                                               db=dbid,
                                               key=&#34;eval&#34;)
        eval = saved_eval[0].to_dataframe()
        saved_baseline = XRSerializer.load_derived(db_root=abs_db_root,
                                                   db=dbid,
                                                   key=&#34;baseline&#34;)
        baseline = saved_baseline[0].to_dataframe()

        # Massage into nice format
        baseline = baseline[[&#34;mean&#34;,&#34;best&#34;]] \
            .groupby(level=[&#34;objective&#34;,&#34;function&#34;]).min()
        baseline = baseline.unstack(level=&#34;objective&#34;)
        baseline.columns = [&#34;visible_baseline&#34;, &#34;generalization_baseline&#34;,
                            &#34;visible_opt&#34;, &#34;generalization_opt&#34;]
        eval = eval.droplevel(&#34;suggestion&#34;)
        eval = eval.reorder_levels([&#34;optimizer&#34;, &#34;function&#34;,
                                    &#34;study_id&#34;, &#34;iter&#34;])
        eval = eval.sort_values(eval.index.names)
        eval = eval.groupby(level=[&#34;optimizer&#34;,&#34;function&#34;,&#34;study_id&#34;]).min()
        eval = eval.rename(columns={&#34;_visible_to_opt&#34;: &#34;visible&#34;})
        eval = eval.rename(columns=lambda x: x + &#34;_achieved&#34;)
        data = eval.join(baseline, on=&#34;function&#34;)

        # Calculate normalized scores
        data[&#34;generalization_score&#34;] = cls._constrain(
            1 - (data[&#34;generalization_achieved&#34;] - data[&#34;generalization_opt&#34;]) \
            / (data[&#34;generalization_baseline&#34;] - data[&#34;generalization_opt&#34;])
        )
        data[&#34;visible_score&#34;] = cls._constrain(
            1 - (data[&#34;visible_achieved&#34;] - data[&#34;visible_opt&#34;]) \
            / (data[&#34;visible_baseline&#34;] - data[&#34;visible_opt&#34;])
        )

        data = data.reindex(sorted(data.columns), axis=1)
        data.columns = data.columns.str.split(&#34;_&#34;, expand=True)

        return data

    @staticmethod
    def _constrain(series):
        return np.maximum(np.minimum(series, 1), 0)</code></pre>
</details>
<h3>Static methods</h3>
<dl>
<dt id="blackboxbandits.compare.BaseOptimizerComparison.get_results_for_dbid"><code class="name flex">
<span>def <span class="ident">get_results_for_dbid</span></span>(<span>dbid: str, db_root: str) ‑> pandas.core.frame.DataFrame</span>
</code></dt>
<dd>
<div class="desc"><p>Get the results of a particular comparison specified by its DBID.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>dbid</code></strong> :&ensp;<code>str</code></dt>
<dd>The DBID of the comparison to get results for.</dd>
<dt><strong><code>db_root</code></strong> :&ensp;<code>str</code></dt>
<dd>Path to the directory containing the folder for
the relevant comparison.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>pd.DataFrame</code></dt>
<dd>A dataframe containing the result of the specified comparison.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@classmethod
def get_results_for_dbid(cls, dbid: str, db_root: str) -&gt; pd.DataFrame:
    &#34;&#34;&#34;Get the results of a particular comparison specified by its DBID.

    Parameters
    ----------
    dbid : str
        The DBID of the comparison to get results for.
    db_root : str
        Path to the directory containing the folder for
        the relevant comparison.
    
    Returns
    -------
    pd.DataFrame
        A dataframe containing the result of the specified comparison.
    &#34;&#34;&#34;
    # Read data saved by Bayesmark
    abs_db_root = os.path.abspath(db_root)
    saved_eval = XRSerializer.load_derived(db_root=abs_db_root,
                                           db=dbid,
                                           key=&#34;eval&#34;)
    eval = saved_eval[0].to_dataframe()
    saved_baseline = XRSerializer.load_derived(db_root=abs_db_root,
                                               db=dbid,
                                               key=&#34;baseline&#34;)
    baseline = saved_baseline[0].to_dataframe()

    # Massage into nice format
    baseline = baseline[[&#34;mean&#34;,&#34;best&#34;]] \
        .groupby(level=[&#34;objective&#34;,&#34;function&#34;]).min()
    baseline = baseline.unstack(level=&#34;objective&#34;)
    baseline.columns = [&#34;visible_baseline&#34;, &#34;generalization_baseline&#34;,
                        &#34;visible_opt&#34;, &#34;generalization_opt&#34;]
    eval = eval.droplevel(&#34;suggestion&#34;)
    eval = eval.reorder_levels([&#34;optimizer&#34;, &#34;function&#34;,
                                &#34;study_id&#34;, &#34;iter&#34;])
    eval = eval.sort_values(eval.index.names)
    eval = eval.groupby(level=[&#34;optimizer&#34;,&#34;function&#34;,&#34;study_id&#34;]).min()
    eval = eval.rename(columns={&#34;_visible_to_opt&#34;: &#34;visible&#34;})
    eval = eval.rename(columns=lambda x: x + &#34;_achieved&#34;)
    data = eval.join(baseline, on=&#34;function&#34;)

    # Calculate normalized scores
    data[&#34;generalization_score&#34;] = cls._constrain(
        1 - (data[&#34;generalization_achieved&#34;] - data[&#34;generalization_opt&#34;]) \
        / (data[&#34;generalization_baseline&#34;] - data[&#34;generalization_opt&#34;])
    )
    data[&#34;visible_score&#34;] = cls._constrain(
        1 - (data[&#34;visible_achieved&#34;] - data[&#34;visible_opt&#34;]) \
        / (data[&#34;visible_baseline&#34;] - data[&#34;visible_opt&#34;])
    )

    data = data.reindex(sorted(data.columns), axis=1)
    data.columns = data.columns.str.split(&#34;_&#34;, expand=True)

    return data</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="blackboxbandits.compare.BaseOptimizerComparison.get_dbid"><code class="name flex">
<span>def <span class="ident">get_dbid</span></span>(<span>self) ‑> int</span>
</code></dt>
<dd>
<div class="desc"><p>Get the unique DBID of this comparison.</p>
<p>This is the name of the folder this comparison's data is saved in.
Must run comparison first.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>int</code></dt>
<dd>The DBID of this experiment.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_dbid(self) -&gt; int:
    &#34;&#34;&#34;Get the unique DBID of this comparison.
    
    This is the name of the folder this comparison&#39;s data is saved in.
    Must run comparison first.

    Returns
    -------
    int
        The DBID of this experiment.
    &#34;&#34;&#34;
    assert self._dbid is not None, &#34;Must run comparison first.&#34;
    return self._dbid</code></pre>
</details>
</dd>
<dt id="blackboxbandits.compare.BaseOptimizerComparison.get_results"><code class="name flex">
<span>def <span class="ident">get_results</span></span>(<span>self) ‑> pandas.core.frame.DataFrame</span>
</code></dt>
<dd>
<div class="desc"><p>Get the results of this comparison as a dataframe.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>pd.DataFrame</code></dt>
<dd>A dataframe containing the results of this comparison.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_results(self) -&gt; pd.DataFrame:
    &#34;&#34;&#34;Get the results of this comparison as a dataframe.

    Returns
    -------
    pd.DataFrame
        A dataframe containing the results of this comparison.
    &#34;&#34;&#34;
    return self.get_results_for_dbid(self._dbid, self.db_root)</code></pre>
</details>
</dd>
<dt id="blackboxbandits.compare.BaseOptimizerComparison.run"><code class="name flex">
<span>def <span class="ident">run</span></span>(<span>self) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Run the comparison experiment defined by this class.</p>
<p>May take a while, depending on whether parallelisation is enabled.</p>
<h2 id="notes">Notes</h2>
<p>If parallelisation is enabled, this generates and uses a temporary
file <code>jobs.txt</code> in the current directory.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def run(self) -&gt; None:
    &#34;&#34;&#34;Run the comparison experiment defined by this class.

    May take a while, depending on whether parallelisation is enabled.

    Notes
    -----

    If parallelisation is enabled, this generates and uses a temporary
    file `jobs.txt` in the current directory.
    &#34;&#34;&#34;
    launcher_args = {
        &#34;-dir&#34;: self.db_root,
        &#34;-o&#34;: &#34; &#34;.join(self.optimizers),
        &#34;-d&#34;: &#34; &#34;.join(self.datasets),
        &#34;-c&#34;: &#34; &#34;.join(self.classifiers),
        &#34;-m&#34;: &#34; &#34;.join(self.metrics),
        &#34;-n&#34;: str(self.num_calls),
        &#34;-r&#34;: str(self.num_repetitions)
    }
    if self.parallel: # Will create list of independent commands to run
        # Approximate number of indep. experiments in this comparison
        launcher_args[&#34;-nj&#34;] = str(len(self.optimizers) * len(self.datasets) \
                             * len(self.classifiers) * self.num_repetitions)

        # Generate dbid for whole batch
        folder_prefix = datetime.utcnow().strftime(&#34;bo_%Y%m%d_%H%M%S_&#34;)
        exp_subdir = mkdtemp(prefix=folder_prefix, dir=self.db_root)
        self._dbid = os.path.basename(exp_subdir)
        launcher_args[&#34;-b&#34;] = self._dbid

        # Setup dbid folder
        for name in [&#34;derived&#34;, &#34;log&#34;, &#34;eval&#34;, &#34;time&#34;, &#34;suggest_log&#34;]:
            os.mkdir(os.path.join(exp_subdir, name))

    # Run Bayesmark experiment launcher
    launcher_command = &#34;bayesmark-launch &#34; + &#34; &#34;.join(
        [key + &#34; &#34; + value for key, value in launcher_args.items()]
    )
    launcher_command += &#34; -v&#34;
    p = subprocess.Popen(launcher_command,
                         stderr=subprocess.PIPE, shell=True)
    while p.poll() is None:
        l = p.stderr.readline().decode(&#34;utf-8&#34;)
        print(l)
        if &#34;Supply --db&#34; in l:
            dbid_string = l

    # If parallel we now need to run the generated commands
    if self.parallel:
        self._run_parallel_commands(self._dbid)
    else:
        # Get DBID
        dbid_start_index = dbid_string.find(&#34;Supply --db&#34;) \
                            + len(&#34;Supply --db&#34;) + 1
        dbid_end_index = dbid_string.find(&#34;to append to this experiment&#34;) - 1
        assert dbid_start_index != -1 and dbid_end_index != -1
        self._dbid = dbid_string[dbid_start_index:dbid_end_index]

    # Aggregate results
    os.system(f&#39;bayesmark-agg -dir &#34;{self.db_root}&#34; -b &#34;{self._dbid}&#34;&#39;)

    # Analyse results (will compute baselines which we&#39;ll use)
    os.system(f&#39;bayesmark-anal -dir &#34;{self.db_root}&#34; -b &#34;{self._dbid}&#34;&#39;)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="blackboxbandits.compare.MetaOptimizerComparison"><code class="flex name class">
<span>class <span class="ident">MetaOptimizerComparison</span></span>
<span>(</span><span>meta_optimizers: Dict[str, <a title="blackboxbandits.meta.AbstractMetaOptimizer" href="meta.html#blackboxbandits.meta.AbstractMetaOptimizer">AbstractMetaOptimizer</a>], base_optimizers: List[str], classifiers: List[str], datasets: List[str], metrics: List[str], num_calls: int, num_repetitions: int, db_root: str, parallel: bool = False, num_workers: Optional[int] = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Interface for comparing <em>combinations</em> of standard black-box
optimizers on ML tasks.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>meta_optimizers</code></strong> :&ensp;<code>Dict[str, meta.AbstractMetaOptimizer]</code></dt>
<dd>Dictionary whose values are the instantiated meta-optimizers to include
in this comparison (these will choose combinations of the base optimizers
defined below) and whose keys are the names used to refer to them.</dd>
<dt><strong><code>base_optimizers</code></strong> :&ensp;<code>List[str]</code></dt>
<dd>List of standard optimizer names known to Bayesmark to be used as the
base optimizers for this comparison.</dd>
<dt><strong><code>classifiers</code></strong> :&ensp;<code>List[str]</code></dt>
<dd>List of sklearn classification methods known to Bayesmark.</dd>
<dt><strong><code>datasets</code></strong> :&ensp;<code>List[str]</code></dt>
<dd>List of classification/regression datasets known to Bayesmark.</dd>
<dt><strong><code>metrics</code></strong> :&ensp;<code>List[str]</code></dt>
<dd>List of loss functions known to Bayesmark. Must include at least one
regression and one classification loss function if <code>datasets</code> includes
both types of task.</dd>
<dt><strong><code>num_calls</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of function evaluations allowed by each optimizer on each task.</dd>
<dt><strong><code>num_repetitions</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of times to repeat the entire experiment for reliability.</dd>
<dt><strong><code>db_root</code></strong> :&ensp;<code>str</code></dt>
<dd>Path to root folder in which a folder for this experiment's data will
be created.</dd>
<dt><strong><code>parallel</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to run the base experiments as a pool of tasks across multiple
worker threads or not. Defaults to False.</dd>
<dt><strong><code>num_workers</code></strong> :&ensp;<code>Optional[int]</code>, optional</dt>
<dd>Number of worker processes to use if parallelisation is enabled. If not
specified defaults to the number of cpu cores.</dd>
</dl>
<h2 id="attributes">Attributes</h2>
<p>Same as parameters.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class MetaOptimizerComparison:
    &#34;&#34;&#34;Interface for comparing *combinations* of standard black-box
    optimizers on ML tasks.

    Parameters
    ----------
    meta_optimizers : Dict[str, meta.AbstractMetaOptimizer]
        Dictionary whose values are the instantiated meta-optimizers to include
        in this comparison (these will choose combinations of the base optimizers
        defined below) and whose keys are the names used to refer to them.
    base_optimizers : List[str]
        List of standard optimizer names known to Bayesmark to be used as the
        base optimizers for this comparison.
    classifiers : List[str]
        List of sklearn classification methods known to Bayesmark.
    datasets : List[str]
        List of classification/regression datasets known to Bayesmark.
    metrics : List[str]
        List of loss functions known to Bayesmark. Must include at least one
        regression and one classification loss function if `datasets` includes
        both types of task.
    num_calls : int
        Number of function evaluations allowed by each optimizer on each task.
    num_repetitions : int
        Number of times to repeat the entire experiment for reliability.
    db_root : str
        Path to root folder in which a folder for this experiment&#39;s data will
        be created.
    parallel : bool, optional
        Whether to run the base experiments as a pool of tasks across multiple
        worker threads or not. Defaults to False.
    num_workers : Optional[int], optional
        Number of worker processes to use if parallelisation is enabled. If not
        specified defaults to the number of cpu cores.

    Attributes
    ----------
    Same as parameters.
    &#34;&#34;&#34;

    def __init__(self,
                 meta_optimizers: Dict[str, AbstractMetaOptimizer],
                 base_optimizers: List[str],
                 classifiers: List[str],
                 datasets: List[str],
                 metrics: List[str],
                 num_calls: int,
                 num_repetitions: int,
                 db_root: str,
                 parallel: bool = False,
                 num_workers: Optional[int] = None):
        self.meta_optimizers = meta_optimizers
        self.base_optimizers = base_optimizers
        self.classifiers = classifiers
        self.datasets = datasets
        self.metrics = metrics
        self.num_calls = num_calls
        self.num_repetitions = num_repetitions
        self.db_root = db_root
        self.parallel = parallel
        self.num_workers = num_workers
        
        self._dbid: Optional[str] = None

        self._meta_comparison_completed = False

    def run_base_comparison(self) -&gt; None:
        &#34;&#34;&#34;Run the base optimizers on the relevant tasks for this experiment.

        May take a while.
        &#34;&#34;&#34;
        base_comparison = BaseOptimizerComparison(
            self.base_optimizers,
            self.classifiers,
            self.datasets,
            self.metrics,
            self.num_calls,
            self.num_repetitions,
            self.db_root,
            self.parallel,
            self.num_workers)
        base_comparison.run()
        self._dbid = base_comparison.get_dbid()
        self._base_comparison_data = base_comparison.get_results()

    def load_base_comparison(self, dbid: str) -&gt; None:
        &#34;&#34;&#34;Load data from a previously run comparison of the base optimizers
        for this experiment.

        The DBID must correspond to a comparison of the correct base optimizers
        for this meta_comparison.

        Parameters
        ----------
        dbid : str
            The DBID for the saved data to be loaded.
        &#34;&#34;&#34;
        self._dbid = dbid
        self._base_comparison_data = BaseOptimizerComparison \
            .get_results_for_dbid(self._dbid, self.db_root)

    def run_meta_comparison(self):
        &#34;&#34;&#34;Run the comparison of meta-optimizers over the base optimizers.

        Must have first run or loaded the base optimizer comparison.
        &#34;&#34;&#34;
        assert self._dbid is not None, &#34;Must run or load base comparison first.&#34;
        self._meta_comparison_completed = True

        all_results = []
        for rep in range(self.num_repetitions):
            results = []
            for meta_optimizer in self.meta_optimizers.values():
                comp_data = self._base_comparison_data.xs(rep, level=&#34;study_id&#34;)
                meta_optimizer.run(comp_data)
                results.append(meta_optimizer.get_results())
            all_results.append(pd.concat(results,
                                         keys=self.meta_optimizers.keys()))
        self._all_meta_results = pd.concat(
            all_results,
            keys=list(range(self.num_repetitions))
        )
        self._all_meta_results.index.rename(
            [&#34;study_id&#34;, &#34;optimizer&#34;, &#34;function&#34;],
            inplace=True
        )
        self._all_meta_results = self._all_meta_results \
            .reorder_levels([&#34;optimizer&#34;, &#34;function&#34;, &#34;study_id&#34;])
        self._all_meta_results = self._all_meta_results \
                                .sort_values(self._all_meta_results.index.names)
    def full_results(self) -&gt; pd.DataFrame:
        &#34;&#34;&#34;Get the full results of this meta-comparison as a dataframe.
        
        Must have run the meta-comparison first.
        
        Returns
        -------
        pd.DataFrame
            A dataframe containing the average visible and generalization scores
            for each base and each meta optimizer, on each task.
        &#34;&#34;&#34;
        assert self._meta_comparison_completed == True, \
             &#34;Must complete comparison before getting results&#34;
        base_results = self._base_comparison_data.xs(&#34;score&#34;, level=1, axis=1) \
                                        .rename(columns=lambda x: x+&#34;_score&#34;)
        return pd.concat([base_results, self._all_meta_results]) \
                .groupby([&#34;optimizer&#34;, &#34;function&#34;]).mean() \
                    [[&#34;visible_score&#34;, &#34;generalization_score&#34;]]

    def summary(self) -&gt; pd.DataFrame:
        &#34;&#34;&#34;Get summarized results of this meta-comparison as a dataframe.
        
        Must have run the meta-comparison first.
        
        Returns
        -------
        pd.DataFrame
            A dataframe containing visible and generalization scores
            for each base and each meta optimizer averaged over all tasks.
        &#34;&#34;&#34;
        return self.full_results().groupby([&#34;optimizer&#34;]).mean()

    def get_dbid(self):
        &#34;&#34;&#34;Get the unique DBID associated with the base optimizer comparison
        for this experiment.

        Must run or load base comparison first.

        Returns
        -------
        int
            The DBID of this base comparison for this experiment.
        &#34;&#34;&#34;
        assert self._dbid is not None, &#34;Must run or load base comparison first.&#34;
        return self._dbid</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="blackboxbandits.compare.MetaOptimizerComparison.full_results"><code class="name flex">
<span>def <span class="ident">full_results</span></span>(<span>self) ‑> pandas.core.frame.DataFrame</span>
</code></dt>
<dd>
<div class="desc"><p>Get the full results of this meta-comparison as a dataframe.</p>
<p>Must have run the meta-comparison first.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>pd.DataFrame</code></dt>
<dd>A dataframe containing the average visible and generalization scores
for each base and each meta optimizer, on each task.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def full_results(self) -&gt; pd.DataFrame:
    &#34;&#34;&#34;Get the full results of this meta-comparison as a dataframe.
    
    Must have run the meta-comparison first.
    
    Returns
    -------
    pd.DataFrame
        A dataframe containing the average visible and generalization scores
        for each base and each meta optimizer, on each task.
    &#34;&#34;&#34;
    assert self._meta_comparison_completed == True, \
         &#34;Must complete comparison before getting results&#34;
    base_results = self._base_comparison_data.xs(&#34;score&#34;, level=1, axis=1) \
                                    .rename(columns=lambda x: x+&#34;_score&#34;)
    return pd.concat([base_results, self._all_meta_results]) \
            .groupby([&#34;optimizer&#34;, &#34;function&#34;]).mean() \
                [[&#34;visible_score&#34;, &#34;generalization_score&#34;]]</code></pre>
</details>
</dd>
<dt id="blackboxbandits.compare.MetaOptimizerComparison.get_dbid"><code class="name flex">
<span>def <span class="ident">get_dbid</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Get the unique DBID associated with the base optimizer comparison
for this experiment.</p>
<p>Must run or load base comparison first.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>int</code></dt>
<dd>The DBID of this base comparison for this experiment.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_dbid(self):
    &#34;&#34;&#34;Get the unique DBID associated with the base optimizer comparison
    for this experiment.

    Must run or load base comparison first.

    Returns
    -------
    int
        The DBID of this base comparison for this experiment.
    &#34;&#34;&#34;
    assert self._dbid is not None, &#34;Must run or load base comparison first.&#34;
    return self._dbid</code></pre>
</details>
</dd>
<dt id="blackboxbandits.compare.MetaOptimizerComparison.load_base_comparison"><code class="name flex">
<span>def <span class="ident">load_base_comparison</span></span>(<span>self, dbid: str) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Load data from a previously run comparison of the base optimizers
for this experiment.</p>
<p>The DBID must correspond to a comparison of the correct base optimizers
for this meta_comparison.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>dbid</code></strong> :&ensp;<code>str</code></dt>
<dd>The DBID for the saved data to be loaded.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_base_comparison(self, dbid: str) -&gt; None:
    &#34;&#34;&#34;Load data from a previously run comparison of the base optimizers
    for this experiment.

    The DBID must correspond to a comparison of the correct base optimizers
    for this meta_comparison.

    Parameters
    ----------
    dbid : str
        The DBID for the saved data to be loaded.
    &#34;&#34;&#34;
    self._dbid = dbid
    self._base_comparison_data = BaseOptimizerComparison \
        .get_results_for_dbid(self._dbid, self.db_root)</code></pre>
</details>
</dd>
<dt id="blackboxbandits.compare.MetaOptimizerComparison.run_base_comparison"><code class="name flex">
<span>def <span class="ident">run_base_comparison</span></span>(<span>self) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Run the base optimizers on the relevant tasks for this experiment.</p>
<p>May take a while.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def run_base_comparison(self) -&gt; None:
    &#34;&#34;&#34;Run the base optimizers on the relevant tasks for this experiment.

    May take a while.
    &#34;&#34;&#34;
    base_comparison = BaseOptimizerComparison(
        self.base_optimizers,
        self.classifiers,
        self.datasets,
        self.metrics,
        self.num_calls,
        self.num_repetitions,
        self.db_root,
        self.parallel,
        self.num_workers)
    base_comparison.run()
    self._dbid = base_comparison.get_dbid()
    self._base_comparison_data = base_comparison.get_results()</code></pre>
</details>
</dd>
<dt id="blackboxbandits.compare.MetaOptimizerComparison.run_meta_comparison"><code class="name flex">
<span>def <span class="ident">run_meta_comparison</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Run the comparison of meta-optimizers over the base optimizers.</p>
<p>Must have first run or loaded the base optimizer comparison.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def run_meta_comparison(self):
    &#34;&#34;&#34;Run the comparison of meta-optimizers over the base optimizers.

    Must have first run or loaded the base optimizer comparison.
    &#34;&#34;&#34;
    assert self._dbid is not None, &#34;Must run or load base comparison first.&#34;
    self._meta_comparison_completed = True

    all_results = []
    for rep in range(self.num_repetitions):
        results = []
        for meta_optimizer in self.meta_optimizers.values():
            comp_data = self._base_comparison_data.xs(rep, level=&#34;study_id&#34;)
            meta_optimizer.run(comp_data)
            results.append(meta_optimizer.get_results())
        all_results.append(pd.concat(results,
                                     keys=self.meta_optimizers.keys()))
    self._all_meta_results = pd.concat(
        all_results,
        keys=list(range(self.num_repetitions))
    )
    self._all_meta_results.index.rename(
        [&#34;study_id&#34;, &#34;optimizer&#34;, &#34;function&#34;],
        inplace=True
    )
    self._all_meta_results = self._all_meta_results \
        .reorder_levels([&#34;optimizer&#34;, &#34;function&#34;, &#34;study_id&#34;])
    self._all_meta_results = self._all_meta_results \
                            .sort_values(self._all_meta_results.index.names)</code></pre>
</details>
</dd>
<dt id="blackboxbandits.compare.MetaOptimizerComparison.summary"><code class="name flex">
<span>def <span class="ident">summary</span></span>(<span>self) ‑> pandas.core.frame.DataFrame</span>
</code></dt>
<dd>
<div class="desc"><p>Get summarized results of this meta-comparison as a dataframe.</p>
<p>Must have run the meta-comparison first.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>pd.DataFrame</code></dt>
<dd>A dataframe containing visible and generalization scores
for each base and each meta optimizer averaged over all tasks.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def summary(self) -&gt; pd.DataFrame:
    &#34;&#34;&#34;Get summarized results of this meta-comparison as a dataframe.
    
    Must have run the meta-comparison first.
    
    Returns
    -------
    pd.DataFrame
        A dataframe containing visible and generalization scores
        for each base and each meta optimizer averaged over all tasks.
    &#34;&#34;&#34;
    return self.full_results().groupby([&#34;optimizer&#34;]).mean()</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="blackboxbandits" href="index.html">blackboxbandits</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="blackboxbandits.compare.BaseOptimizerComparison" href="#blackboxbandits.compare.BaseOptimizerComparison">BaseOptimizerComparison</a></code></h4>
<ul class="">
<li><code><a title="blackboxbandits.compare.BaseOptimizerComparison.get_dbid" href="#blackboxbandits.compare.BaseOptimizerComparison.get_dbid">get_dbid</a></code></li>
<li><code><a title="blackboxbandits.compare.BaseOptimizerComparison.get_results" href="#blackboxbandits.compare.BaseOptimizerComparison.get_results">get_results</a></code></li>
<li><code><a title="blackboxbandits.compare.BaseOptimizerComparison.get_results_for_dbid" href="#blackboxbandits.compare.BaseOptimizerComparison.get_results_for_dbid">get_results_for_dbid</a></code></li>
<li><code><a title="blackboxbandits.compare.BaseOptimizerComparison.run" href="#blackboxbandits.compare.BaseOptimizerComparison.run">run</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="blackboxbandits.compare.MetaOptimizerComparison" href="#blackboxbandits.compare.MetaOptimizerComparison">MetaOptimizerComparison</a></code></h4>
<ul class="">
<li><code><a title="blackboxbandits.compare.MetaOptimizerComparison.full_results" href="#blackboxbandits.compare.MetaOptimizerComparison.full_results">full_results</a></code></li>
<li><code><a title="blackboxbandits.compare.MetaOptimizerComparison.get_dbid" href="#blackboxbandits.compare.MetaOptimizerComparison.get_dbid">get_dbid</a></code></li>
<li><code><a title="blackboxbandits.compare.MetaOptimizerComparison.load_base_comparison" href="#blackboxbandits.compare.MetaOptimizerComparison.load_base_comparison">load_base_comparison</a></code></li>
<li><code><a title="blackboxbandits.compare.MetaOptimizerComparison.run_base_comparison" href="#blackboxbandits.compare.MetaOptimizerComparison.run_base_comparison">run_base_comparison</a></code></li>
<li><code><a title="blackboxbandits.compare.MetaOptimizerComparison.run_meta_comparison" href="#blackboxbandits.compare.MetaOptimizerComparison.run_meta_comparison">run_meta_comparison</a></code></li>
<li><code><a title="blackboxbandits.compare.MetaOptimizerComparison.summary" href="#blackboxbandits.compare.MetaOptimizerComparison.summary">summary</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>